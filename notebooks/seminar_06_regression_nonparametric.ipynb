{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\Sum}{\\sum\\limits}\n",
    "\\newcommand{\\Int}{\\int\\limits}\n",
    "\\newcommand{\\Intf}{\\int\\limits_{-\\infty}^{+\\infty}}\n",
    "\\newcommand{\\Prod}{\\prod\\limits}\n",
    "\\newcommand{\\Max}{\\max\\limits}\n",
    "\\newcommand{\\Min}{\\min\\limits}\n",
    "\\newcommand{\\Lim}{\\lim\\limits}\n",
    "\\newcommand{\\Var}{\\mathbb{V}}\n",
    "\\newcommand{\\Exp}{\\mathbb{E}}\n",
    "\\newcommand{\\argmax}{\\arg\\max}\n",
    "\\newcommand{\\Cov}{\\mathrm{Cov}}\n",
    "\\newcommand{\\makebold}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\mean}[1]{\\overline{#1}}\n",
    "\\newcommand{\\avg}[1]{\\left\\langle #1 \\right\\rangle}\n",
    "\\newcommand{\\Prob}{\\mathcal{P}}\n",
    "\\newcommand{\\lp}{\\left}\n",
    "\\newcommand{\\rp}{\\right}\n",
    "\\newcommand{\\eps}{\\varepsilon}\n",
    "\\newcommand{\\loss}{\\mathcal{L}}\n",
    "\\newcommand{\\Llr}{\\mathcal{L}}\n",
    "\\newcommand{\\llr}{\\ell}\n",
    "\\newcommand{\\RR}{\\mathbb{R}}\n",
    "\\newcommand{\\ZZ}{\\mathbb{Z}}\n",
    "\\newcommand{\\NN}{\\mathbb{N}}\n",
    "\\newcommand{\\Poisson}{\\mathrm{Poisson}}\n",
    "\\newcommand{\\Uniform}{\\mathrm{Uniform}}\n",
    "\\newcommand{\\Binomial}{\\mathrm{Binomial}}\n",
    "\\newcommand{\\Gammap}{\\mathrm{Gamma}}\n",
    "\\newcommand{\\Normal}{\\mathcal{N}}\n",
    "\\newcommand{\\LogN}{\\mathrm{LogN}}\n",
    "\\newcommand{\\Exponential}{\\mathrm{Exp}}\n",
    "\\newcommand{\\Erlang}{\\mathrm{Erlang}}\n",
    "\\newcommand{\\Cauchy}{C}\n",
    "\\newcommand{\\boldtheta}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\boldtau}{\\boldsymbol{\\tau}}\n",
    "\\newcommand{\\boldX}{\\boldsymbol{X}}\n",
    "\\newcommand{\\boldY}{\\boldsymbol{Y}}\n",
    "\\newcommand{\\boldZ}{\\boldsymbol{Z}}\n",
    "\\newcommand{\\boldtheta}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\boldtau}{\\boldsymbol{\\tau}}\n",
    "\\newcommand{\\boldp}{\\boldsymbol{p}}\n",
    "\\newcommand{\\boldu}{\\boldsymbol{u}}\n",
    "\\newcommand{\\boldv}{\\boldsymbol{v}}\n",
    "\\newcommand{\\boldw}{\\boldsymbol{w}}\n",
    "\\newcommand{\\boldx}{\\boldsymbol{x}}\n",
    "\\newcommand{\\boldy}{\\boldsymbol{y}}\n",
    "\\newcommand{\\boldz}{\\boldsymbol{z}}\n",
    "\\newcommand{\\lf}{\\left\\{}\n",
    "\\newcommand{\\rf}{\\right\\}}\n",
    "\\newcommand{\\lp}{\\left(}\n",
    "\\newcommand{\\rp}{\\right)}\n",
    "\\newcommand{\\ls}{\\left[}\n",
    "\\newcommand{\\rs}{\\right]}\n",
    "\\newcommand{\\lv}{\\left|}\n",
    "\\newcommand{\\rv}{\\right|}\n",
    "\\newcommand{\\Ecdf}[1]{\\hat{F}_n(#1)}\n",
    "\\newcommand{\\boot}{\\mathrm{boot}}\n",
    "\\newcommand{\\bias}{\\mathrm{bias}}\n",
    "\\newcommand{\\se}{\\mathrm{se}}\n",
    "\\newcommand{\\MSE}{\\mathrm{MSE}}\n",
    "\\newcommand{\\MLE}{\\mathrm{MLE}}\n",
    "\\newcommand{\\qm}{\\mathrm{qm}}\n",
    "\\newcommand{\\as}{\\mathrm{as}}\n",
    "\\newcommand{\\trace}{\\mathrm{trace}}\n",
    "\\newcommand{\\esttheta}{\\hat{\\theta}}\n",
    "\\newcommand{\\estlambda}{\\hat{\\lambda}}\n",
    "\\newcommand{\\estmu}{\\hat{\\mu}}\n",
    "\\newcommand{\\estsigma}{\\hat{\\sigma}}\n",
    "\\newcommand{\\estalpha}{\\hat{\\alpha}}\n",
    "\\newcommand{\\estbeta}{\\hat{\\beta}}\n",
    "\\newcommand{\\estxi}{\\hat{\\xi}}\n",
    "\\newcommand{\\esttau}{\\hat{\\tau}}\n",
    "\\newcommand{\\estpsi}{\\hat{\\psi}}\n",
    "\\newcommand{\\esta}{\\hat{a}}\n",
    "\\newcommand{\\estb}{\\hat{b}}\n",
    "\\newcommand{\\estc}{\\hat{c}}\n",
    "\\newcommand{\\estd}{\\hat{d}}\n",
    "\\newcommand{\\estp}{\\hat{p}}\n",
    "\\newcommand{\\estT}{\\hat{T}}\n",
    "\\newcommand{\\estR}{\\hat{R}}\n",
    "\\newcommand{\\estF}{\\hat{F}}\n",
    "\\newcommand{\\estf}{\\hat{f}}\n",
    "\\newcommand{\\estC}{\\hat{C}}\n",
    "\\newcommand{\\estS}{\\hat{S}}\n",
    "\\newcommand{\\estY}{\\hat{Y}}\n",
    "\\newcommand{\\esty}{\\hat{y}}\n",
    "\\newcommand{\\estVar}{\\hat{\\Var}}\n",
    "\\newcommand{\\estExp}{\\hat{\\Exp}}\n",
    "\\newcommand{\\estSe}{\\hat{\\se}}\n",
    "\\newcommand{\\hata}{\\hat{a}}\n",
    "\\newcommand{\\hatb}{\\hat{b}}\n",
    "\\newcommand{\\hatc}{\\hat{c}}\n",
    "\\newcommand{\\hatd}{\\hat{d}}\n",
    "\\newcommand{\\hatf}{\\hat{f}}\n",
    "\\newcommand{\\hatg}{\\hat{g}}\n",
    "\\newcommand{\\hatk}{\\hat{k}}\n",
    "\\newcommand{\\hatp}{\\hat{p}}\n",
    "\\newcommand{\\hatr}{\\hat{r}}\n",
    "\\newcommand{\\hatt}{\\hat{t}}\n",
    "\\newcommand{\\haty}{\\hat{y}}\n",
    "\\newcommand{\\hatw}{\\hat{w}}\n",
    "\\newcommand{\\hatC}{\\hat{C}}\n",
    "\\newcommand{\\hatF}{\\hat{F}}\n",
    "\\newcommand{\\hatJ}{\\hat{J}}\n",
    "\\newcommand{\\hatK}{\\hat{K}}\n",
    "\\newcommand{\\hatY}{\\hat{Y}}\n",
    "\\newcommand{\\hatsigma}{\\hat{\\sigma}}\n",
    "\\newcommand{\\hatboldw}{\\hat{\\boldw}}\n",
    "\\newcommand{\\tiltau}{\\tilde{\\tau}}\n",
    "\\newcommand{\\tiltheta}{\\tilde{\\theta}}\n",
    "\\newcommand{\\tillambda}{\\tilde{\\lambda}}\n",
    "\\newcommand{\\tilsigma}{\\tilde{\\sigma}}\n",
    "\\newcommand{\\mlexi}{\\xi_{MLE}}\n",
    "\\newcommand{\\mletheta}{\\theta_{MLE}}\n",
    "\\newcommand{\\mlelambda}{\\lambda_{MLE}}\n",
    "\\newcommand{\\mlesigma}{\\sigma_{MLE}}\n",
    "\\newcommand{\\mmxi}{\\xi_{MM}}\n",
    "\\newcommand{\\mmtheta}{\\theta_{MM}}\n",
    "\\newcommand{\\mmlambda}{\\lambda_{MM}}\n",
    "\\newcommand{\\mmsigma}{\\sigma_{MM}}\n",
    "\\newcommand{\\mmgamma}{\\gamma_{MM}}\n",
    "\\newcommand{\\xs}[1]{\\boldx^{(#1)}}\n",
    "\\newcommand{\\ys}[1]{\\boldy^{(#1)}}\n",
    "\\newcommand{\\zs}[1]{\\boldz^{(#1)}}\n",
    "\\newcommand{\\Xs}[1]{\\boldX^{(#1)}}\n",
    "\\newcommand{\\Ys}[1]{\\boldY^{(#1)}}\n",
    "\\newcommand{\\Zs}[1]{\\boldZ^{(#1)}}\n",
    "\\newcommand{\\mcalL}{\\mathcal{L}}\n",
    "\\newcommand{\\mcalF}{\\mathcal{F}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import copy\n",
    "\n",
    "from scipy import stats\n",
    "from itertools import product\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from itertools import combinations, product\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "# sklearn\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.kernel_ridge import KernelRidge, pairwise_kernels\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "%matplotlib inline\n",
    "\n",
    "titlesize = 24\n",
    "labelsize = 22\n",
    "legendsize = 22\n",
    "xticksize = 18\n",
    "yticksize = xticksize\n",
    "\n",
    "matplotlib.rcParams['legend.markerscale'] = 1.5     # the relative size of legend markers vs. original\n",
    "matplotlib.rcParams['legend.handletextpad'] = 0.5\n",
    "matplotlib.rcParams['legend.labelspacing'] = 0.4    # the vertical space between the legend entries in fraction of fontsize\n",
    "matplotlib.rcParams['legend.borderpad'] = 0.5       # border whitespace in fontsize units\n",
    "matplotlib.rcParams['font.size'] = 12\n",
    "matplotlib.rcParams['font.family'] = 'serif'\n",
    "matplotlib.rcParams['font.serif'] = 'Times New Roman'\n",
    "matplotlib.rcParams['axes.labelsize'] = labelsize\n",
    "matplotlib.rcParams['axes.titlesize'] = titlesize\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 8)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=xticksize)\n",
    "matplotlib.rc('ytick', labelsize=yticksize)\n",
    "matplotlib.rc('legend', fontsize=legendsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc('font', **{'family':'serif'})\n",
    "matplotlib.rc('text', usetex=True)\n",
    "matplotlib.rc('text.latex', unicode=True)\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage[english]{babel}')\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage{amsmath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = !pwd\n",
    "curr_dir = curr_dir.get_nlstr()\n",
    "repo_dir, notebooks_dir = os.path.split(curr_dir)\n",
    "assert notebooks_dir == 'notebooks'\n",
    "DATASETS_DIR = os.path.join(repo_dir, 'datasets')\n",
    "LIBRARY_DIR = os.path.join(repo_dir, 'asml')\n",
    "\n",
    "if not LIBRARY_DIR in sys.path:\n",
    "    sys.path.append(LIBRARY_DIR)\n",
    "if not DATASETS_DIR in sys.path:\n",
    "    sys.path.append(DATASETS_DIR)\n",
    "\n",
    "print(LIBRARY_DIR)\n",
    "print(DATASETS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='_toc'></a>\n",
    "# Содержание\n",
    "* [Непараметрическая оценка плотности распределения](#nonparam_pdf_est)\n",
    "* [Гистограмная оценка плотности (histogram estimator)](#nonparam_pdf_hist)\n",
    "* [Ядерная оценка плотности (kernel density estimator)](#nonparam_pdf_kernel)\n",
    "* [Непараметрическая регрессия](#nonparam_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='nonparam_pdf_est'></a>\n",
    "# Непараметрическая оценка плотности распределения<sup>[toc](#_toc)</sup>\n",
    "(nonparametric density estimation)\n",
    "\n",
    "* Гистограмная оценка\n",
    "* Ядерная оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='nonparam_pdf_est_quality'></a>\n",
    "## Качество оценки плотности распределения<sup>[toc](#_toc)</sup>\n",
    "\n",
    "* $F$ &mdash; неизвестное непрерывное распределение с плотностью $p(x)$.\n",
    "* $\\xs{n} \\triangleq \\{x_1,\\dots,x_n\\}$ &mdash; выборка из распределения $F$, т.е. $\\xs{n} \\sim F$.\n",
    "* $\\hatp_n(x) = \\hatp(x;\\xs{n})$ &mdash; оценка плотности $p(x)$, построенная по выборке $\\xs{n}$.\n",
    "* $\\hatp$ &mdash; алгоритм построения оценки по выборке $\\xs{n}$.\n",
    "\n",
    "Как оценить качество оценки $\\hatp(x;\\xs{n})$?\n",
    "\n",
    "Рассмотрим в качестве функции потерь L2-метрику между $p(x)$ и $\\hatp_n(x)$:\n",
    "$$\n",
    "L(p,\\hatp;\\xs{n}) \\triangleq \\rho_{L_2}(p(x),\\hatp(x;\\xs{n})) = \\int (p(x) - \\hatp_n(x))^2 dx.\n",
    "$$\n",
    "Она называется **integrated squared error (ISE)**.\n",
    "Фактически при заданном распределении $F$ ISE является случайной величиной, так как является функцией от выборки $\\xs{n}$. Нас интересует способность рассматривамого алгоритма $\\hatp_n$ аппроксимировать истинную плотность $p(x)$ на всевозможных выборках $\\xs{n}$, т.е. то, как данный алгоритм ведем себя в среднем. Поэтому в качестве ошибки будем рассматривать мат. ожидание от $L(p,\\hatp;\\xs{n})$.\n",
    "\n",
    "**Риском** называется усредненная L2-норма (mean integrated squared error / MISE):\n",
    "$$\n",
    "R(p,\\hatp; n) = \\Exp_{\\xs{n}\\sim F}[L(p,\\hatp;\\xs{n})].\n",
    "$$\n",
    "\n",
    "Обозначим через $\\overline{p}_n(x)$ усредненную оценку плотности:\n",
    "$$\n",
    "\\overline{p}_n(x) = \\Exp_{\\xs{n} \\sim F} [\\hatp_n(x)] = \\Exp_{\\xs{n}\\sim F} [\\hatp(x;\\xs{n})].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bias-variance risk decomposition<sup>[toc](#_toc)</sup>\n",
    "\\begin{gather*}\n",
    "R(p,\\hatp; n) = \\Exp_{\\xs{n}\\sim F} \\Bigl[ \\int (p(x) - \\hatp_n(x))^2 dx \\Bigr] \n",
    "= \\Exp_{\\xs{n}\\sim F} \\Bigl[ \\int (p(x) - \\overline{p}_n(x))^2 + (\\overline{p}_n(x) - \\hatp_n(x))^2 dx + 2 \\int (p(x) - \\overline{p}_n(x))(\\overline{p}_n(x) - \\hatp_n(x)) dx\\Bigr]\n",
    "= \\Exp_{\\xs{n}\\sim F} \\Bigl[ \\int (p(x) - \\overline{p}_n(x))^2 dx \\Bigr] + \\Exp_{\\xs{n}\\sim F} \\Bigl[\\int (\\overline{p}_n(x) - \\hatp_n(x))^2  dx \\Bigr] + 2 \\Exp_{\\xs{n}\\sim F} \\Bigl[ \\int (p(x) - \\overline{p}_n(x))(\\overline{p}_n(x) - \\hatp_n(x)) dx\\Bigr] = \\\\\n",
    "= \\int \\underbrace{(p(x) - \\overline{p}_n(x))^2}_{\\text{bias}^2(x)} dx  + \\int \\underbrace{\\Exp_{\\xs{n}\\sim F}  \\bigl[(\\overline{p}_n(x) - \\hatp_n(x))^2\\bigr]}_{\\text{variance}(x)}  dx\n",
    "\\end{gather*}\n",
    "\n",
    "Таким образом, получаем, что\n",
    "\\begin{align}\n",
    "&R(p,\\hatp, n) = \\Int \\text{bias}(x)^2 dx + \\Int \\text{variance}(x) dx,\\\\\n",
    "&\\text{bias}(x) = \\overline{p}_n(x) - p(x), \\\\\n",
    "&\\text{variance}(x) = \\Exp_{\\xs{n}\\sim F}  \\bigl[(\\hatp_n(x) - \\overline{p}_n(x))^2\\bigr].\n",
    "\\end{align}\n",
    "\n",
    "При анализе алгоритма $\\hatp$ обычно анализируют то, как ведет себя риск $R(p,\\hatp; n)$ с ростом размера выборки $n$. Приведем асимптотики для некоторых методов, рассматриваемых далее:\n",
    "* Гистограммная оценка:\n",
    "$$\n",
    "R(p,\\hatp; n) \\approx \\frac{\\text{const}}{n^{2/3}}\n",
    "$$\n",
    "* Ядерная оценка\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation estimator of risk<sup>[toc](#_toc)</sup>\n",
    "На практике мы не можем явно подсчитать ни $L(p,\\hatp_n)$, ни тем более $R(p,\\hatp_n)$, так как обе величины зависят от неизвестной плотности $p(x)$. Однако можем попытаться оценить значение $L(p,\\hatp_n)$.\n",
    "\n",
    "$$\n",
    "L(p,\\hatp;\\xs{n}) = \\Int (p(x) - \\hatp(x;\\xs{n}))^2 dx =  \\Int p(x)^2 dx  - 2 \\Int p(x) \\hatp(x;\\xs{n})d x +\n",
    "\\Int \\hatp(x;\\xs{n})^2 dx\n",
    "$$\n",
    "Первое слагаемое является константой, а потому не представляет интереса с точки зрения минимизации $L(p,\\hatp;\\xs{n})$. Поэтому далее будем в качестве фукнции ошибки рассматривать значение $J(p,\\hatp;\\xs{n})$:\n",
    "\\begin{gather*}\n",
    "J(p,\\hatp;\\xs{n}) = \\int \\hatp(x;\\xs{n})^2 dx - 2 \\int \\hatp(x;\\xs{n}) p(x) dx\n",
    "\\end{gather*}\n",
    "Здесь $\\hatp(x;\\xs{n})$ нам известна, а $p(x)$ нет. Поэтому требуется как-то оценить значение $J(p,\\hatp;\\xs{n})$ только по выборке $\\xs{n}$. Для этого посмотрим на смысл выражения\n",
    "$$\n",
    "\\int \\hatp(x;\\xs{n}) p(x) dx\n",
    "$$\n",
    "Пусть $X \\sim F(x)$, где $F(x)$ &mdash; функция распределения, соответствующая плотности $p(x)$. Пусть выборка \n",
    "$\\xs{n}$ фиксирована. Рассмотрим случайную величину $Z = \\hatp(Z;\\xs{n})$. Тогда\n",
    "$$\n",
    "\\Exp Z = \\int \\hatp(x;\\xs{n}) p(x) dx.\n",
    "$$\n",
    "Забудем на секунду об определении величины $Z$ и будем считать ее просто некоторой случайной величиной. По закону больших чисел мы можем оценить среднее $Z$ по выборке $\\boldZ^{(n)} = \\lf Z_1, \\dots, Z_n \\rf$ следующим образом:\n",
    "$$\n",
    "\\Exp Z \\approx \\frac{1}{n} \\Sum_{i = 1}^n Z_i.\n",
    "$$\n",
    "В нашем случае казалось бы, должны записать\n",
    "$$\n",
    "\\Exp Z \\approx \\frac{1}{n} \\Sum_{i = 1}^n \\hatp(x_i;\\xs{n}).\n",
    "$$\n",
    "Но тут интуитивно возникает подозрение, что $\\hatp(x_i;\\xs{n})$ будет некоторой смещенной оценкой из-за наличия $x_i$ в выборке $\\xs{n}$, поэтому для чистоты эксперимента следует убрать ее из $\\xs{n}$ при вычислении $\\Exp Z$:\n",
    "$$\n",
    "\\Exp Z \\approx \\frac{1}{n} \\Sum_{i = 1}^n \\hatp(x_i;\\xs{n}).\n",
    "$$\n",
    "Идея состоит в том, чтобы устранить эту зависимость: случайная величина $\\hatp(X;\\xs{n})$ зависит от $\\xs{n}$, и по значениям в этих же точках (а других значений у нас нет) мы должны как-то оценить ее среднее. Для этого предлагается исключить значение $x_i$ из выборки $\\xs{n}$ при построении оценки плостности $\\hatp(x;\\cdot)$:\n",
    "$$\n",
    "\\Exp Z \\approx \\frac{1}{n} \\Sum_{i = 1}^n \\hatp(x_i;\\xs{n\\backslash i}).\n",
    "$$\n",
    "\n",
    "К тому же можно прийти и другим путем, предположив, что у нас есть еще одна выборка $\\xs{m}$, отличная от \n",
    "$\\xs{n}_*$, так что\n",
    "$$\n",
    "\\Exp Z \\approx \\frac{1}{n} \\Sum_{j = 1}^m \\hatp(x_{*,j};\\xs{n}).\n",
    "$$\n",
    "\n",
    "Таким образом, предлагается в качестве оценки $\\hatJ(p,\\hatp;\\xs{n})$ взять\n",
    "\\begin{gather*}\n",
    "\\boxed{\\hatJ(p,\\hatp;\\xs{n}) = \\int \\hatp(x;\\xs{n})^2 dx - \\frac{2}{n}\\Sum_{i = 1}^n \\hatp(x_i;\\xs{n\\backslash i}) dx.}\n",
    "\\end{gather*}\n",
    "Велична $\\hatJ(p,\\hatp;\\xs{n})$ называется **cross-validation score of estimated risk**.\n",
    "\n",
    "---\n",
    "**Теорема.** Cross-validation score is nearly unbiased.\n",
    "\\begin{gather*}\n",
    "\\boxed{\\Exp_{\\xs{n}\\sim F} [\\hatJ(p,\\hatp,\\xs{n})] \\approx \\Exp_{\\xs{n} \\sim F}[J(p,\\hatp, \\xs{n})].}\n",
    "\\end{gather*}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='nonparam_pdf_hist'></a>\n",
    "# Гистограмная оценка плотности<sup>[toc](#_toc)</sup>\n",
    "* [Теория](#hist_pdf_theory)\n",
    "    * [Описание метода](#hist_pdf_estimation)\n",
    "    * [Оценка риска](#hist_pdf_risk)\n",
    "* [Примеры](#hist_pdf_examples)\n",
    "    * [Смесь двух нормальных распределений](#hist_pdf_gm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='hist_pdf_theory'></a>\n",
    "## Теория<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='hist_pdf_estimation'></a>\n",
    "### Гистограмная оценка плотности<sup>[toc](#_toc)</sup>\n",
    "\n",
    "Гистограмная оценка плотности имеет вид\n",
    "$$\n",
    "\\hatp(x;\\xs{n}) = \\frac{1}{nh}\\Sum_{m = 1}^M n_m I[x \\in B_m] = \\frac{1}{h}\\Sum_{m = 1}^M \\hatp_m I[x \\in B_m],\n",
    "$$\n",
    "где\n",
    "$$\n",
    "n_m = \\Sum_{i = 1}^n I[x_i \\in B_m], \\qquad \\hatp_m = \\frac{n_m}{n}.\n",
    "$$\n",
    "Несложно заметить, что в числителе $\\hatp_m$ стоит биномиальная случайная величина $n_m$ (относительно выборки $\\xs{n}$), которая имеет следующие параметры:\n",
    "$$\n",
    "n_m \\sim \\Binomial(n, p_m), \\quad p_m = \\Int_{B_m} p(x) dx.\n",
    "$$\n",
    "Следовательно\n",
    "\\begin{align*}\n",
    "&\\Exp [\\hatp_m] = \\frac{\\Exp n_m}{n} = \\frac{n p_m}{n} = p_m,\\\\\n",
    "&\\Var [\\hatp_m] = \\frac{1}{n^2}\\Var n_m = \\frac{p_m(1 - p_m)}{n},\\\\\n",
    "&\\Exp [\\hatp_m^2] = \\Var \\hatp_m + \\lp\\Exp \\hatp_m\\rp^2 = \\frac{p_m(1 - p_m)}{n} + p_m^2.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='hist_pdf_risk'></a>\n",
    "### Оценка риска<sup>[toc](#_toc)</sup>\n",
    "\n",
    "В случае гистограмной оценки можно показать, что оценка $\\hatJ(h)$ для риска\n",
    "\\begin{gather*}\n",
    "J(h) = \\int \\hatp (x;\\xs{n})^2dx - \\frac{2}{n}\\Sum_{i = 1}^n \\hatp(x_i;\\xs{n\\backslash i})\n",
    "\\end{gather*}\n",
    "имеет вид\n",
    "\\begin{gather*}\n",
    "\\hatJ(h) = \\frac{2}{(n - 1)h} - \\frac{n+1}{(n-1)h}\\Sum_{m = 1}^M \\hatp_m^2,\\quad \\hatp_m = \\frac{n_m}{n}.\n",
    "\\end{gather*}\n",
    "Остается выбрать такую ширину $h$, что $\\hatJ(h)$ имеет минимальное значение.\n",
    "\n",
    "**Вопрос:** каково отличие между $\\Exp[J(h)]$ и $\\Exp[\\hatJ(h)]$ в случае гистограмной оценки $\\hatp(x;\\xs{n})$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binedges(center, bandwidth, x_left, x_right):\n",
    "    \"\"\"\n",
    "    Функция для удобного создания бинов заданной ширины и с заданным положением центра одного из бинов\n",
    "    :param center: Значение центра одного из бинов\n",
    "    :param bandwidth: Ширина бина\n",
    "    :param x_left: Значение левой границы самого левого бина не больше x_left\n",
    "    :param x_right: Значение правой границы самого правого бина не меньше x_right\n",
    "    \"\"\"\n",
    "    bins = [center - bandwidth / 2]\n",
    "    while x_left < bins[-1]:\n",
    "        bins.append(bins[-1] - bandwidth)\n",
    "    bins = list(reversed(bins))\n",
    "    while x_right > bins[-1]:\n",
    "        bins.append(bins[-1] + bandwidth)\n",
    "    return bins\n",
    "\n",
    "\n",
    "def histogram_pdf_estimated_risk(bincounts, bandwidth=None, n_bins=None):\n",
    "    h = bandwidth\n",
    "    n = np.sum(bincounts)\n",
    "    ps = bincounts / n\n",
    "    return 2 / ((n - 1) * h) - ((n + 1.0) / (h * (n - 1.0))) * np.sum(ps ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='hist_pdf_examples'></a>\n",
    "## Примеры<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='hist_pdf_gm'></a>\n",
    "### Смесь двух нормальных распреденеий<sup>[toc](#_toc)</sup>\n",
    "$$\n",
    "p(x) = \\frac{p_1}{\\sqrt{2\\pi}\\sigma_1}e^{-\\frac{(x - \\mu_1)^2}{2\\sigma_1^2}} + \\frac{p_2}{\\sqrt{2\\pi}\\sigma_2}e^{-\\frac{(x - \\mu_2)^2}{2\\sigma_2^2}}, \\qquad p_1 + p_2 = 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание выборки из смеси нормальных распределений<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal densities mixture parameters\n",
    "mu1 = 0\n",
    "mu2 = 5\n",
    "sigma1 = 1\n",
    "sigma2 = 1\n",
    "p1 = 0.3\n",
    "p2 = 1 - p1\n",
    "\n",
    "# Samples parameters\n",
    "seed = 1\n",
    "n_samples = 100\n",
    "\n",
    "gen1 = scipy.stats.norm(loc=mu1, scale=sigma1)\n",
    "gen2 = scipy.stats.norm(loc=mu2, scale=sigma2)\n",
    "\n",
    "samples1 = gen1.rvs(size=int(p1 * n_samples), random_state=seed)       # Samples from the first components\n",
    "samples2 = gen2.rvs(size=int(p2 * n_samples), random_state=seed + 1)   # Samples from the second components\n",
    "samples = np.concatenate([samples1, samples2])                         # All samples\n",
    "\n",
    "x_values = np.linspace(-5, 10, 1000)\n",
    "binedges = np.linspace(-5, 10, 10)\n",
    "true_pdf = p1 * gen1.pdf(x_values) + p2 * gen2.pdf(x_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF для некоторой ширины бина<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = 0.2\n",
    "binedges = create_binedges(0., bandwidth, samples.min(), samples.max())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.hist(samples, bins=binedges, color='b', density=True, edgecolor='k', \n",
    "         label='estimated pdf', alpha=0.5, zorder=2)\n",
    "plt.plot(x_values, true_pdf, color='r', label='true pdf', zorder=2)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.legend()\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция $\\hatJ(h)$ оценки риска<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth_range = np.linspace(0.01, 10, 1000)\n",
    "risk_values = []\n",
    "for bandwidth in bandwidth_range:\n",
    "    binedges = create_binedges(0., bandwidth, samples.min(), samples.max())\n",
    "    bincounts, _ = np.histogram(samples, bins=binedges)\n",
    "    risk_values.append(histogram_pdf_estimated_risk(bincounts, bandwidth))\n",
    "risk_values = np.array(risk_values)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(bandwidth_range, risk_values, color='b', label=r'$\\hat{J}(h)$', zorder=2);\n",
    "plt.xlabel(r'$h$'); plt.ylabel(r'$\\hat{J}(h)$')\n",
    "plt.legend();\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the optimal bandwidth\n",
    "index_min = np.argmin(risk_values)\n",
    "opt_bandwidth = bandwidth_range[index_min]\n",
    "min_risk = risk_values[index_min]\n",
    "print('h = {}, J = {}'.format(opt_bandwidth, min_risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF для оптимальной ширины бина<sup>[toc](#_toc)</sup>\n",
    "Теперь посмотрим, как выглядит гистограмная оценка $\\hatp(x;h,\\xs{n})$ при оптимальной ширине бина $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binedges = create_binedges(0, opt_bandwidth, samples.min(), samples.max())\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.hist(samples, bins=binedges, color='b', density=True, edgecolor='k', \n",
    "         label='estimated pdf', alpha=0.5, zorder=2)\n",
    "plt.plot(x_values, true_pdf, color='r', label='true pdf', zorder=2)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.legend()\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='hist_pdf_sdss'></a>\n",
    "### Sloan Digital Sky Survey<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(os.path.join(DATASETS_DIR, 'galaxy.dat.txt'), sep='\\s+', header=None)\n",
    "data[:4]\n",
    "samples = data[2].to_numpy()\n",
    "samples = samples[samples < 0.2]\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF для некоторой ширины бина<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = 0.01\n",
    "binedges = create_binedges(0., bandwidth, samples.min(), samples.max())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.hist(samples, bins=binedges, color='b', density=True, edgecolor='k', \n",
    "         label='estimated pdf', alpha=0.5, zorder=2)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.legend()\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция $\\hatJ(h)$ оценки риска<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth_range = np.linspace(0.0001, 0.2, 20000)\n",
    "bins_numbers = []\n",
    "risk_values = []\n",
    "for bandwidth in bandwidth_range:\n",
    "    binedges = create_binedges(0., bandwidth, samples.min(), samples.max())\n",
    "    bincounts, _ = np.histogram(samples, bins=binedges)\n",
    "    bins_numbers.append(len(binedges) - 1)\n",
    "    risk_values.append(histogram_pdf_estimated_risk(bincounts, bandwidth))\n",
    "risk_values = np.array(risk_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(1, 2, figsize=(15, 8))\n",
    "ax = axarr[0]\n",
    "ax.plot(bins_numbers, risk_values, color='b', label=r'$\\hat{J}(B)$', zorder=2);\n",
    "ax.set_xlabel(r'$B$')\n",
    "ax.set_ylabel(r'$\\hat{J}(B)$')\n",
    "ax.legend();\n",
    "ax.grid(which='both', linestyle='--', alpha=0.5);\n",
    "\n",
    "ax = axarr[1]\n",
    "ax.plot(bandwidth_range, risk_values, color='b', label=r'$\\hat{J}(h)$', zorder=2);\n",
    "ax.set_xlabel(r'$h$')\n",
    "ax.set_ylabel(r'$\\hat{J}(h)$')\n",
    "ax.set_xscale('log')\n",
    "ax.legend();\n",
    "ax.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the optimal bandwidth\n",
    "index_min = np.argmin(risk_values)\n",
    "opt_bandwidth = bandwidth_range[index_min]\n",
    "min_risk = risk_values[index_min]\n",
    "print('h = {}, J = {}'.format(opt_bandwidth, min_risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF для оптимальной ширины бина<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binedges = create_binedges(0, opt_bandwidth, samples.min(), samples.max())\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.hist(samples, bins=binedges, color='b', density=True, edgecolor='k', \n",
    "         label='estimated pdf', alpha=0.5, zorder=2)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.legend()\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nonparam_pdf_kernel'></a>\n",
    "## Ядерная оценка плотности<sup>[toc](#_toc)</sup>\n",
    "* [Ядерная оценка плотности. Риск](#kernel_risk)\n",
    "* [Simple 1D Kernel Density Estimation](#kernel_pdf_ex1)\n",
    "* [Kernel Density Estimation](#kernel_pdf_digits)\n",
    "* [Примеры](#kernel_pdf_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kernel_risk'></a>\n",
    "### Ядерная оценка плотности. Риск<sup>[toc](#_toc)<sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ядерная оценка плотности имеет вид\n",
    "$$\n",
    "\\hatp(x;\\xs{n}) = \\frac{1}{n} \\Sum_{i = 1}^n \\frac{1}{h}K\\lp\\frac{x - x_i}{h}\\rp,\n",
    "$$\n",
    "где\n",
    "* $K(\\cdot)$ &mdash; неотрицательная симметричная относительно нуля функция такая, что\n",
    "$$\n",
    "\\Int K(y)dy = 1.\n",
    "$$\n",
    "* $h$ &mdash; ширина ядра.\n",
    "\n",
    "* При $h \\to 0$ получим, что оценка $\\hatp(x;\\xs{n})$ состоит из \"пиков\" в точках выборки $\\xs{n}$:\n",
    "$$\n",
    "\\hatp(x;\\xs{n}) = \\Sum_{i = 1}^n \\delta(x - x_i).\n",
    "$$\n",
    "* При $h \\to \\infty$ получим, что оценка $\\hatp(x;\\xs{n})$ равномерна на $\\RR$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция риска в самом общем случае имеет вид\n",
    "\\begin{gather*}\n",
    "J(p,\\hatp;\\xs{n}) = \\int \\hatp(x;\\xs{n})^2 dx - 2 \\int \\hatp(x;\\xs{n}) p(x) dx.\n",
    "\\end{gather*}\n",
    "Оценка этой функции по выборке $\\xs{n}$ имеет вид\n",
    "\\begin{gather*}\n",
    "\\hatJ(p,\\hatp;\\xs{n}) = \\int \\hatp(x;\\xs{n})^2 dx - \\frac{2}{n} \\Sum_{i=1}^n \\hatp(x_i;\\xs{n\\backslash i}).\n",
    "\\end{gather*}\n",
    "Можно показать, что в случае ядерной оценки плотности\n",
    "$$\n",
    "\\Exp J(p,\\hatp;\\xs{n}) = \\Exp \\hatJ(p,\\hatp;\\xs{n}).\n",
    "$$\n",
    "\n",
    "При вычислениях воспользуемся следующей аппроксимацией оценки риска:\n",
    "\\begin{gather*}\n",
    "\\boxed{\\hatJ(h) \\approx \\frac{1}{hn^2}\\Sum_{i = 1}^n\\Sum_{j = 1}^n K^{(2)}\\lp\\frac{x_i - x_j}{h}\\rp + \\frac{2K(0)}{nh},}\n",
    "\\end{gather*}\n",
    "где\n",
    "\\begin{gather*}\n",
    "K^{(2)}(x) = K^*(x) - 2K(x), \\quad K^*(x) = \\int K(x - y) K(y) dy.\n",
    "\\end{gather*}\n",
    "\n",
    "Далее приводятся примеры [ядерное оценивание плотности из библиотеки sklearn](http://scikit-learn.org/stable/modules/density.html).\n",
    "\n",
    "* Gaussian kernel\n",
    "$$\n",
    "K(x;h) = \\frac{1}{\\sqrt{2\\pi}h}\\exp\\lp -\\frac{x^2}{2h^2}\\rp \\cdot I[x \\in (-h, h)].\n",
    "$$\n",
    "* Tophat kernel\n",
    "$$\n",
    "K(x;h) = \\frac{1}{2h}\\cdot I[x \\in (-h, h)]\n",
    "$$\n",
    "* Epanechnikov kernel\n",
    "$$\n",
    "K(x;h) = \\frac{3}{8h}\\lp 1 - \\frac{x^2}{h^2}\\rp \\cdot I[x \\in (-h, h)]\n",
    "$$\n",
    "* Exponential kernel\n",
    "$$\n",
    "K(x;h) = \\frac{1}{2h}\\exp \\lp -\\frac{|x|}{h} \\rp \\cdot I[x \\in (-h, h)]\n",
    "$$\n",
    "* Linear kernel\n",
    "$$\n",
    "K(x;h) = \\frac{1}{h}\\lp 1 - \\frac{|x|}{h} \\rp \\cdot I[x \\in (-h, h)]\n",
    "$$\n",
    "* Cosine kernel\n",
    "$$\n",
    "K(x;h) = \\frac{4h}{\\pi} \\cos\\lp\\frac{\\pi x}{2 h}\\rp \\cdot I[x \\in (-h, h)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_estimated_risk(samples, bandwidth):\n",
    "    \"\"\"Calculates the risk for gaussian kernel\n",
    "\n",
    "    :param samples: 1D numpy array with samples\n",
    "    :param bandwidth: bandwidth parameter of gaussian kernel\n",
    "\n",
    "    :rtype: float\n",
    "    :returns: Estimated risk value\n",
    "    \"\"\"\n",
    "    X = samples\n",
    "    h = bandwidth\n",
    "    n = len(X)\n",
    "    Jh = 0\n",
    "    Xi = X[None, :]\n",
    "    Xj = X[:, None]\n",
    "    points = (Xi - Xj) / h\n",
    "    Jh = np.sum(stats.norm.pdf(points, loc=0, scale=np.sqrt(2))) -\\\n",
    "        2 * np.sum(stats.norm.pdf(points, loc=0, scale=1))\n",
    "    Jh /= h * (n**2)\n",
    "    Jh += (2.0 / (h * n)) * stats.norm.pdf(0, loc=0, scale=1)\n",
    "    return Jh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kernel_pdf_ex1'></a>\n",
    "### Simple 1D Kernel Density Estimation<sup>[toc](#toc)<sup>\n",
    "This example uses the `sklearn.neighbors.KernelDensity` class to demonstrate the principles of Kernel Density Estimation in one dimension.\n",
    "\n",
    "The first plot shows one of the problems with using histograms to visualize the density of points in 1D. Intuitively, a histogram can be thought of as a scheme in which a unit \"block\" is stacked above each point on a regular grid.\n",
    "As the top two panels show, however, the choice of gridding for these blocks can lead to wildly divergent ideas about the underlying shape of the density distribution.  If we instead center each block on the point it represents, we\n",
    "get the estimate shown in the bottom left panel.  This is a kernel density estimation with a \"top hat\" kernel.  This idea can be generalized to other kernel shapes: the bottom-right panel of the first figure shows a Gaussian kernel density estimate over the same distribution.\n",
    "\n",
    "Scikit-learn implements efficient kernel density estimation using either a Ball Tree or KD Tree structure, through the `sklearn.neighbors.KernelDensity` estimator.  The available kernels are shown in the second figure of this example.\n",
    " \n",
    "The third figure compares kernel density estimates for a distribution of 100 samples in 1 dimension.  Though this example uses 1D distributions, kernel density estimation is easily and efficiently extensible to higher dimensions\n",
    "as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Визуализация гистограмных и ядровых оценок<sup>[toc](#toc)<sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "n_samples = 100\n",
    "\n",
    "mu1 = 0\n",
    "mu2 = 5\n",
    "sigma1 = 1\n",
    "sigma2 = 1\n",
    "p1 = 0.3\n",
    "p2 = 1 - p1\n",
    "\n",
    "gen1 = scipy.stats.norm(loc=mu1, scale=sigma1)\n",
    "gen2 = scipy.stats.norm(loc=mu2, scale=sigma2)\n",
    "\n",
    "samples1 = gen1.rvs(size=int(p1 * n_samples), random_state=seed)       # Samples from the first components\n",
    "samples2 = gen2.rvs(size=int(p2 * n_samples), random_state=seed + 1)   # Samples from the second components\n",
    "samples = np.concatenate([samples1, samples2])                         # All samples\n",
    "\n",
    "# Finding the boundaries to plot\n",
    "alpha = 0.01\n",
    "x_left = np.percentile(samples, 100 * alpha)\n",
    "x_right = np.percentile(samples, 100 * (1 - alpha))\n",
    "x_width = x_right - x_left\n",
    "gamma = 0.25\n",
    "x_left -= gamma * x_width\n",
    "x_right += gamma * x_width\n",
    "\n",
    "x_values = np.linspace(-5, 10, 1000)\n",
    "binedges = np.linspace(-5, 10, 10)\n",
    "\n",
    "true_pdf = p1 * gen1.pdf(x_values) + p2 * gen2.pdf(x_values)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(15, 15))\n",
    "\n",
    "# histogram 1\n",
    "ax[0, 0].hist(samples, bins=binedges, fc='#AAAAFF', density=True, edgecolor='k')\n",
    "ax[0, 0].set_title(\"Histogram\")\n",
    "\n",
    "# histogram 2\n",
    "ax[0, 1].hist(samples, bins=binedges + 0.75, fc='#AAAAFF', density=True, edgecolor='k')\n",
    "ax[0, 1].set_title(\"Histogram, bins shifted\")\n",
    "\n",
    "# Tophat KDE\n",
    "kde = KernelDensity(kernel='tophat', bandwidth=0.75).fit(samples[:,None])\n",
    "log_dens = kde.score_samples(x_values[:,None])\n",
    "ax[1, 0].plot(x_values, np.exp(log_dens), color='k', zorder=2)\n",
    "ax[1, 0].fill(x_values, np.exp(log_dens), fc='#AAAAFF', zorder=2)\n",
    "ax[1, 0].set_title(\"Tophat Kernel Density\")\n",
    "\n",
    "# Gaussian KDE\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.75).fit(samples[:,None])\n",
    "log_dens = kde.score_samples(x_values[:,None])\n",
    "ax[1, 1].plot(x_values, np.exp(log_dens), color='k', zorder=2)\n",
    "ax[1, 1].fill(x_values, np.exp(log_dens), fc='#AAAAFF')\n",
    "ax[1, 1].set_title(\"Gaussian Kernel Density\")\n",
    "\n",
    "for n_row, n_col in product(range(2), range(2)):\n",
    "    ax[n_row, n_col].grid(which='both', linestyle='--', alpha=0.5)\n",
    "    # Plotting true PDF on each subplot\n",
    "    ax[n_row, n_col].plot(x_values, true_pdf, color='r', linestyle='--', zorder=2);\n",
    "    \n",
    "for axi in ax.ravel():\n",
    "    axi.plot(samples, np.zeros(len(samples)) - 0.01, '+k')\n",
    "    axi.set_xlim(x_left, x_right)\n",
    "    axi.set_ylim(-0.02, 0.34)\n",
    "\n",
    "for axi in ax[:, 0]:\n",
    "    axi.set_ylabel('Normalized Density')\n",
    "\n",
    "for axi in ax[1, :]:\n",
    "    axi.set_xlabel('$x$')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Отрисовка различных ядер<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-6, 6, 1000)[:, None]\n",
    "X_src = np.zeros((1, 1))\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(12, 6))\n",
    "#fig.subplots_adjust(left=0.05, right=0.95, hspace=0.05, wspace=0.05)\n",
    "\n",
    "def format_func(x, loc):\n",
    "    if x == 0:\n",
    "        return '0'\n",
    "    elif x == 1:\n",
    "        return 'h'\n",
    "    elif x == -1:\n",
    "        return '-h'\n",
    "    else:\n",
    "        return '%ih' % x\n",
    "\n",
    "for i, kernel in enumerate(['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine']):\n",
    "    axi = ax.ravel()[i]\n",
    "    log_dens = KernelDensity(kernel=kernel).fit(X_src).score_samples(x_values)\n",
    "    axi.plot(x_values[:, 0], np.exp(log_dens), color='k', zorder=2);\n",
    "    axi.fill(x_values[:, 0], np.exp(log_dens), '-k', fc='#AAAAFF');\n",
    "    axi.set_title(kernel)\n",
    "\n",
    "    axi.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "    axi.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    axi.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    axi.set_ylim(0, 1.05)\n",
    "    axi.set_xlim(-2.9, 2.9)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аппроксимация одномерной плотности помощью различных ядер<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "n_samples = 100\n",
    "gen1 = scipy.stats.norm(loc=0, scale=1)\n",
    "gen2 = scipy.stats.norm(loc=5, scale=1)\n",
    "p1 = 0.3\n",
    "p2 = 1 - p1\n",
    "samples1 = gen1.rvs(size=int(p1 * n_samples), random_state=seed)       # Samples from the first components\n",
    "samples2 = gen2.rvs(size=int(p2 * n_samples), random_state=seed + 1)   # Samples from the second components\n",
    "samples = np.concatenate([samples1, samples2])                         # All samples\n",
    "\n",
    "x_values = np.linspace(-5, 10, 1000)\n",
    "binedges = np.linspace(-5, 10, 10)\n",
    "true_pdf = p1 * gen1.pdf(x_values) + p2 * gen2.pdf(x_values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.fill(x_values, true_pdf, fc='black', alpha=0.2, label='input distribution')\n",
    "\n",
    "for kernel in ['gaussian', 'tophat', 'epanechnikov']:\n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(samples[:,None])\n",
    "    log_dens = kde.score_samples(x_values[:, None])\n",
    "    ax.plot(x_values, np.exp(log_dens), '-', label=\"kernel = ``{0}\\\"\".format(kernel))\n",
    "\n",
    "ax.set_title(\"N={0} points\".format(n_samples))\n",
    "ax.legend(loc='upper left')\n",
    "ax.plot(samples, -0.005 - 0.01 * np.random.random(samples.shape[0]), '+k')\n",
    "\n",
    "ax.set_xlim(-4, 9)\n",
    "ax.set_ylim(-0.02, 0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kernel_pdf_digits'></a>  \n",
    "## Kernel Density Estimation<sup>[toc](#_toc)</sup>\n",
    "\n",
    "This example shows how kernel density estimation (KDE), a powerful non-parametric density estimation technique, can be used to learn a generative model for a dataset. With this generative model in place, new samples can be drawn.  These new samples reflect the underlying model of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "digits = load_digits()\n",
    "data = digits.data\n",
    "# targets = digits.target\n",
    "print('Initial data shape: data.shape = {}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the 64-dimensional data to a lower dimension\n",
    "pca = PCA(n_components=15, whiten=False)\n",
    "data = pca.fit_transform(digits.data)\n",
    "print('After dimensionality reduction: data.shape = {}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use grid search cross-validation to optimize the bandwidth\n",
    "params = {'bandwidth': np.logspace(-1, 1, 20)}\n",
    "grid = GridSearchCV(KernelDensity(), params)\n",
    "grid.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n",
    "\n",
    "# use the best estimator to compute the kernel density estimate\n",
    "kde = grid.best_estimator_\n",
    "\n",
    "# sample 44 new points from the data\n",
    "new_data = kde.sample(44, random_state=0)\n",
    "new_data = pca.inverse_transform(new_data)\n",
    "\n",
    "# turn data into a 4x11 grid\n",
    "new_data = new_data.reshape((4, 11, -1))\n",
    "real_data = digits.data[:44].reshape((4, 11, -1))\n",
    "\n",
    "# plot real digits and resampled digits\n",
    "fig, ax = plt.subplots(9, 11, subplot_kw=dict(xticks=[], yticks=[]))\n",
    "for j in range(11):\n",
    "    ax[4, j].set_visible(False)\n",
    "    for i in range(4):\n",
    "        im = ax[i, j].imshow(real_data[i, j].reshape((8, 8)),\n",
    "                             cmap=plt.cm.binary, interpolation='nearest')\n",
    "        im.set_clim(0, 16)\n",
    "        im = ax[i + 5, j].imshow(new_data[i, j].reshape((8, 8)),\n",
    "                                 cmap=plt.cm.binary, interpolation='nearest')\n",
    "        im.set_clim(0, 16)\n",
    "\n",
    "ax[0, 5].set_title('Selection from the input data')\n",
    "ax[5, 5].set_title('``New\" digits drawn from the kernel density model')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='kernel_pdf_examples'></a>\n",
    "## Примеры<sup>[toc](#_toc)</sup>\n",
    "* [Смесь нормальных распределений](#kernel_pdf_examples_gm)\n",
    "* []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='kernel_pdf_examples_gm'></a>\n",
    "## Смесь нормальных распределений<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание выборки<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "n_samples = 200\n",
    "gen1 = scipy.stats.norm(loc=0, scale=1)\n",
    "gen2 = scipy.stats.norm(loc=5, scale=1)\n",
    "p1 = 0.3\n",
    "p2 = 1 - p1\n",
    "samples1 = gen1.rvs(size=int(p1 * n_samples), random_state=seed)       # Samples from the first components\n",
    "samples2 = gen2.rvs(size=int(p2 * n_samples), random_state=seed + 1)   # Samples from the second components\n",
    "samples = np.concatenate([samples1, samples2])                         # All samples\n",
    "X = samples\n",
    "\n",
    "x_values = np.linspace(-5, 10, 1000)\n",
    "true_pdf = p1 * gen1.pdf(x_values) + p2 * gen2.pdf(x_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF для некоторой ширины ядра<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KernelDensity(bandwidth=0.5)\n",
    "kde.fit(X[:, None])\n",
    "log_probas = kde.score_samples(x_values[:, None])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(x_values, np.exp(log_probas), color='b', label='kernel pdf')\n",
    "plt.plot(x_values, true_pdf, color='r', label='true pdf')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$p(x)$')\n",
    "plt.grid(which='both', alpha=0.7, linestyle='--')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция $\\hatJ(h)$ оценки риска<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth_range = np.linspace(0.01, 10, 100)\n",
    "risk_values = []\n",
    "for bandwidth in bandwidth_range:\n",
    "    risk_values.append(gaussian_kernel_estimated_risk(samples, bandwidth))\n",
    "risk_values = np.array(risk_values)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(bandwidth_range, risk_values, color='b', label=r'$\\hat{J}(h)$', zorder=2);\n",
    "plt.legend();\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_min = np.argmin(risk_values)\n",
    "opt_bandwidth = bandwidth_range[index_min]\n",
    "min_risk = risk_values[index_min]\n",
    "print('h = {}, J = {}'.format(opt_bandwidth, min_risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF для оптимальной ширины ядра<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KernelDensity(bandwidth=opt_bandwidth)\n",
    "kde.fit(samples[:,None])\n",
    "pdf = np.exp(kde.score_samples(x_values[:,None]))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(x_values, pdf, color='b', label='estimated pdf', zorder=2);\n",
    "plt.plot(x_values, true_pdf, color='r', label='true pdf', zorder=2);\n",
    "plt.legend();\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kernel_pdf_species'></a>\n",
    "## X. Kernel Density Estimate of Species Distributions<sup>[toc](#_toc)</sup>\n",
    "\n",
    "This shows an example of a neighbors-based query (in particular a kernel\n",
    "density estimate) on geospatial data, using a Ball Tree built upon the\n",
    "Haversine distance metric -- i.e. distances over points in latitude/longitude.\n",
    "The dataset is provided by Phillips et. al. (2006).\n",
    "If available, the example uses\n",
    "`basemap <http://matplotlib.org/basemap>`_\n",
    "to plot the coast lines and national boundaries of South America.\n",
    "\n",
    "This example does not perform any learning over the data\n",
    "(see `sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py` for\n",
    "an example of classification based on the attributes in this dataset).  It\n",
    "simply shows the kernel density estimate of observed data points in\n",
    "geospatial coordinates.\n",
    "\n",
    "The two species are:\n",
    "\n",
    " - `\"Bradypus variegatus\"\n",
    "   <http://www.iucnredlist.org/apps/redlist/details/3038/0>`_ ,\n",
    "   the Brown-throated Sloth.\n",
    "\n",
    " - `\"Microryzomys minutus\"\n",
    "   <http://www.iucnredlist.org/details/13408/0>`_ ,\n",
    "   also known as the Forest Small Rice Rat, a rodent that lives in Peru,\n",
    "   Colombia, Ecuador, Peru, and Venezuela.\n",
    "\n",
    "**References**\n",
    "\n",
    "\n",
    " * `\"Maximum entropy modeling of species geographic distributions\"\n",
    "   <http://rob.schapire.net/papers/ecolmod.pdf>`_\n",
    "   S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,\n",
    "   190:231-259, 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_species_distributions\n",
    "from sklearn.datasets.species_distributions import construct_grids\n",
    "\n",
    "# if basemap is available, we'll use it.\n",
    "# otherwise, we'll improvise later...\n",
    "try:\n",
    "    from mpl_toolkits.basemap import Basemap\n",
    "    basemap = True\n",
    "except ImportError:\n",
    "    basemap = False\n",
    "\n",
    "# Get matrices/arrays of species IDs and locations\n",
    "data = fetch_species_distributions()\n",
    "species_names = ['Bradypus Variegatus', 'Microryzomys Minutus']\n",
    "\n",
    "Xtrain = np.vstack([data['train']['dd lat'],\n",
    "                    data['train']['dd long']]).T\n",
    "ytrain = np.array([d.decode('ascii').startswith('micro')\n",
    "                  for d in data['train']['species']], dtype='int')\n",
    "Xtrain *= np.pi / 180.  # Convert lat/long to radians\n",
    "\n",
    "# Set up the data grid for the contour plot\n",
    "xgrid, ygrid = construct_grids(data)\n",
    "X, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\n",
    "land_reference = data.coverages[6][::5, ::5]\n",
    "land_mask = (land_reference > -9999).ravel()\n",
    "\n",
    "xy = np.vstack([Y.ravel(), X.ravel()]).T\n",
    "xy = xy[land_mask]\n",
    "xy *= np.pi / 180.\n",
    "\n",
    "# Plot map of South America with distributions of each species\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n",
    "\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "\n",
    "    # construct a kernel density estimate of the distribution\n",
    "    print(\" - computing KDE in spherical coordinates\")\n",
    "    kde = KernelDensity(bandwidth=0.04, metric='haversine',\n",
    "                        kernel='gaussian', algorithm='ball_tree')\n",
    "    kde.fit(Xtrain[ytrain == i])\n",
    "\n",
    "    # evaluate only on the land: -9999 indicates ocean\n",
    "    Z = -9999 + np.zeros(land_mask.shape[0])\n",
    "    Z[land_mask] = np.exp(kde.score_samples(xy))\n",
    "    Z = Z.reshape(X.shape)\n",
    "\n",
    "    # plot contours of the densЬity\n",
    "    levels = np.linspace(0, Z.max(), 25)\n",
    "    plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n",
    "\n",
    "    if basemap:\n",
    "        print(\" - plot coastlines using basemap\")\n",
    "        m = Basemap(projection='cyl', llcrnrlat=Y.min(),\n",
    "                    urcrnrlat=Y.max(), llcrnrlon=X.min(),\n",
    "                    urcrnrlon=X.max(), resolution='c')\n",
    "        m.drawcoastlines()\n",
    "        m.drawcountries()\n",
    "    else:\n",
    "        print(\" - plot coastlines from coverage\")\n",
    "        plt.contour(X, Y, land_reference,\n",
    "                    levels=[-9999], colors=\"k\",\n",
    "                    linestyles=\"solid\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "    plt.title(species_names[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='nonparam_regr'></a>\n",
    "# Непараметрическая регрессия<sup>[toc](#_toc)</sup>\n",
    "* [Теория](#nonparam_theory)\n",
    "    * []\n",
    "* [Примеры](#nonparam_examples)\n",
    "    * [Синусоида 1](#example_sinusoid)\n",
    "    * [Синусоида 2](#example_sinusoid2)\n",
    "    * [LiDAR](#nonparam_regr_example_lidar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='nonparam_theory'></a>\n",
    "## Теория<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='nonparam_theory_nw'></a>\n",
    "### Формула Надарая-Ватсона<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть дана выборка $(x_1,y_1), \\dots, (x_n, y_n)$, где $x \\in \\RR$ и $y \\in \\RR$. Обозначим $\\boldx = \\{x_1,\\dots,x_n\\}$, $\\boldy = \\{y_1,\\dots,y_n\\}$.\n",
    "\n",
    "Требуется построить оценку функции регрессии $r(x) = \\Exp[y|x]$. Если нам известно совместное распределение $p(x,y)$, то\n",
    "$$\n",
    "r(x) = \\int y p(y|x) dy = \\frac{\\int y p(x, y) dy}{p(x)dx} = \\frac{\\int y p(x, y) dy}{\\int p(x, y')dy'}.\n",
    "$$\n",
    "Возьмем в качестве $p(x, y)$ её ядерную оценку:\n",
    "$$\n",
    "\\hatp(x) = \\frac{1}{n h_X h_Y} \\Sum_{i=1}^n K_X\\lp \\frac{x - x_i}{h_X} \\rp K_Y\\lp \\frac{y - y_i}{h_Y}\\rp.\n",
    "$$\n",
    "В таком случае\n",
    "\\begin{align}\n",
    "&\\int \\hatp(x, y) dy = \\frac{1}{n h_X} \\Sum_{i=1}^n K_X\\lp \\frac{x - x_i}{h_X} \\rp,\\\\\n",
    "&\\int y \\hatp(x, y) dy = \\frac{1}{n h_X} \\Sum_{i=1}^n y_i K_X\\lp \\frac{x - x_i}{h_X} \\rp.\n",
    "\\end{align}\n",
    "В результате для получаем:\n",
    "$$\n",
    "\\hat{r}(x;\\boldx,\\boldy) = \\frac{\\sum_{j=1}^n y_j K\\lp\\frac{x - x_i}{h}\\rp }{\\sum_{j=1}^n K\\lp\\frac{x - x_j}{h}\\rp}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Пусть $K(x)$ &mdash; ядро. **Ядерная оценка Надарая-Ватсона для функции регрессии $r(x)$**, построенная по выборке $(\\boldx,\\boldy)$, имеет вид:\n",
    "\\begin{gather*}\n",
    "\\hat{r}(x;\\boldx,\\boldy) = \\Sum_{i=1}^n w(x_i) y_i,\n",
    "\\end{gather*}\n",
    "где\n",
    "$$\n",
    "w(x_i) = \\frac{K\\lp\\frac{x - x_i}{h}\\rp}{\\sum_{j=1}^n K\\lp\\frac{x - x_j}{h}\\rp}.\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Sum_i w_i(x) \\lp a(x) - y_i\\rp^2 \\to min_{a(x)},\\quad K_i(x) = K\\lp \\frac{x - x_i}{h} \\rp\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='nonparam_theory_risk'></a>\n",
    "### Оценка риска<sup>[toc](#_toc)</sup>\n",
    "На практике для оценки параметра $h$ минимизируют следующую оценку риска\n",
    "\\begin{gather*}\n",
    "\\hatJ(h) = \\Sum_{i = 1}^n (y_i - r(x_i;\\xs{n\\backslash i},\\ys{n\\backslash i}))^2.\n",
    "\\end{gather*}\n",
    "Можно показать, что\n",
    "$$\n",
    "\\hatJ(h)\n",
    "= \\Sum_{i = 1}^{n} \\lf \\lp y_i - \\hat{r}(x_i) \\rp^2 \\cdot \\lp 1 - \\frac{K(0)}{\\sum_{j=1}^n K_{ij}} \\rp^{-2} \\rf\n",
    "= \\Sum_{i = 1}^{n} \\lf \\lp y_i - \\frac{\\sum_{j} y_j K_{ij}}{\\sum_{j}K_{ij}}\\rp^{2} \\cdot \\lp 1 - \\frac{K(0)}{\\sum_{j=1}^n K_{ij}} \\rp^{-2} \\rf.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernel:\n",
    "    def __init__(self, loc=0, scale=1):\n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return stats.norm.pdf(x, loc=self.loc, scale=self.scale)\n",
    "\n",
    "\n",
    "class NadarayaWatsonRegressor:\n",
    "    def __init__(self, kernel, bandwidth):\n",
    "        self.kernel = kernel\n",
    "        self.bandwidth = bandwidth\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.X = np.array(X)\n",
    "        self.Y = np.array(Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        K_X = (X[:, None] - self.X[None, :]) / self.bandwidth\n",
    "        K = self.kernel(K_X)\n",
    "        predictions = np.sum(K * Y[None, :], axis=1) / np.sum(K, axis=1)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def nw_regression_estimated_risk(X, Y, bandwidth, kernel=None):\n",
    "    if kernel is None:\n",
    "        kernel = GaussianKernel()\n",
    "    h = bandwidth\n",
    "    nw_regressor = NadarayaWatsonRegressor(GaussianKernel(), h)\n",
    "    nw_regressor.fit(X, Y)\n",
    "    Y_pred = nw_regressor.predict(X)\n",
    "    K_X = (X.reshape((-1, 1)) - X.reshape((1, -1))) / h\n",
    "    K = kernel(K_X)\n",
    "    K_sums = np.sum(K, axis=1)\n",
    "    J = np.sum((((Y - Y_pred) * K_sums) / (K_sums - kernel(0)))**2)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='nonparam_examples'></a>\n",
    "# Примеры<sup>[toc](#_toc)</sup>\n",
    "* [Синусоида 1](#example_sinusoid)\n",
    "* [Синусоида 2](#example_sinusoid2)\n",
    "* [LiDAR](#nonparam_regr_example_lidar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='example_sinusoid'></a>\n",
    "## Синусоида 1<sup>[toc](#_toc)</sup>\n",
    "* [Создание данных](#example_sinusoid_data)\n",
    "* [Регрессия Надарая-Ватсона](#example_sinusoid_nw)\n",
    "* [Выбор оптимальной ширины ядра](#example_sinusoid_opt)\n",
    "* [Построение доверительной трубки](#example_sinusoid_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='example_sinusoid_data'></a>\n",
    "### Создание данных<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "n_samples = 200\n",
    "gen1 = scipy.stats.norm(loc=0, scale=1)\n",
    "gen2 = scipy.stats.norm(loc=5, scale=2)\n",
    "p1 = 0.3\n",
    "p2 = 1 - p1\n",
    "\n",
    "# Creating artificial data\n",
    "samples1 = gen1.rvs(size=int(p1 * n_samples), random_state=seed)       # Samples from the first components\n",
    "samples2 = gen2.rvs(size=int(p2 * n_samples), random_state=seed + 1)   # Samples from the second components\n",
    "X = np.concatenate([samples1, samples2])                               # All samples\n",
    "\n",
    "np.random.seed(seed + 3)\n",
    "Y = np.sin(X) + np.random.normal(loc=0, scale=0.5, size=(n_samples,))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X, Y, color='b', alpha=0.5)\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='example_sinusoid_nw'></a>\n",
    "### Регрессия Надарая-Ватсона<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = 1.\n",
    "nw_regressor = NadarayaWatsonRegressor(GaussianKernel(), bandwidth)\n",
    "indices = np.argsort(X)\n",
    "X = X[indices]\n",
    "Y = Y[indices]\n",
    "nw_regressor.fit(X, Y)\n",
    "Y_pred = nw_regressor.predict(X)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(X, Y_pred, color='b', label='$\\hat{{r}}(x)$ by NW with $h = {}$'.format(bandwidth), zorder=2)\n",
    "plt.plot(X, Y, color='g', marker='o', markersize=4, linestyle='none', label=r'$r(x) + \\varepsilon$ (train)',\n",
    "         zorder=2)\n",
    "plt.plot(x_values, np.sin(x_values), color='r', label='$r(x)$')\n",
    "plt.legend()\n",
    "plt.xlabel('$x$');\n",
    "plt.ylabel('$y$');\n",
    "x_span = X.max() - X.min()\n",
    "y_span = Y.max() - Y.min() \n",
    "plt.xlim([X.min() - 0.1 * x_span, X.max() + 0.1 * x_span])\n",
    "plt.ylim([Y.min() - 0.2 * y_span, Y.max() + 0.2 * y_span])\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='example_sinusoid_opt'></a>\n",
    "### Выбор оптимальной ширины ядра<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth_values = np.linspace(0.1, 2, 191)\n",
    "\n",
    "estimated_risks = []\n",
    "for bandwidth in bandwidth_values:\n",
    "    risk = nw_regression_estimated_risk(X, Y, bandwidth, kernel=GaussianKernel())\n",
    "    estimated_risks.append(risk)\n",
    "\n",
    "bandwidth_opt = bandwidth_values[np.argmin(estimated_risks)]\n",
    "print(f'h={bandwidth_opt}, J(h)={estimated_risks[np.argmin(estimated_risks)]}')\n",
    "\n",
    "plt.plot(bandwidth_values, estimated_risks)\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='example_sinusoid_conf'></a>\n",
    "### Построение доверительной трубки<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.95\n",
    "h = bandwidth_opt\n",
    "n = len(X)\n",
    "\n",
    "a = X.min()\n",
    "b = X.max()\n",
    "m = (b - a) / (3 * h)\n",
    "q = scipy.stats.norm.ppf((1 + (1 - alpha)**(1 / m)) / 2.)\n",
    "sigma = np.sqrt(np.sum((Y[1:] - Y[:-1])**2) / (2 * (n - 1)))\n",
    "\n",
    "x_values = np.linspace(a, b, 1001)\n",
    "kernel = GaussianKernel()\n",
    "K = kernel((x_values[:, None] - X[None, :]) / h)\n",
    "W = K / K.sum(axis=1)[:, None]\n",
    "se = sigma * np.sqrt(np.sum(W ** 2, axis=1))\n",
    "\n",
    "Y_pred = np.sum(W * Y[None, :], axis=1)\n",
    "lower = Y_pred - q * se\n",
    "upper = Y_pred + q * se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.fill_between(x_values, lower, upper, color='b', alpha=0.5, zorder=2)\n",
    "plt.plot(x_values, lower, color='k', linewidth=1, linestyle='--')\n",
    "plt.plot(x_values, upper, color='k', linewidth=1, linestyle='--')\n",
    "plt.plot(x_values, Y_pred, color='r', label=r'$\\hat{r}(x)$')\n",
    "plt.plot(X, Y, color='g', marker='o', markersize=4, linestyle='none', label=r'$r(x) + \\varepsilon$ (train)')\n",
    "\n",
    "# Limits\n",
    "x_span = X.max() - X.min()\n",
    "y_span = Y.max() - Y.min() \n",
    "plt.xlim([X.min() - 0.1 * x_span, X.max() + 0.1 * x_span])\n",
    "plt.ylim([Y.min() - 0.2 * y_span, Y.max() + 0.2 * y_span])\n",
    "\n",
    "# Labels\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('95\\% confidence band')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(which='both', alpha=0.5, linestyle='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='example_sinusoid2'></a>\n",
    "## Синусоида 2<sup>[toc](#_toc)</sup>\n",
    "* [Создание выборки](#ex_kernel_pdf_sin_create)\n",
    "* [Первичная оценка функции регрессии](#ex_kernel_pdf_sin_first)\n",
    "* [Оптимальная ширина ядра](#ex_kernel_pdf_sin_opt)\n",
    "* [Построение доверительной трубки](#ex_kernel_pdf_sin_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex_kernel_pdf_sin_create'></a>\n",
    "### Создание выборки<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "# Generate sample data\n",
    "X = 15 * rng.rand(100)\n",
    "Y = np.sin(X).ravel()\n",
    "Y += 3 * (0.5 - rng.rand(X.shape[0]))  # add noise\n",
    "\n",
    "x_values = np.linspace(X.min(), X.max(), num=1000)\n",
    "y_values = np.sin(x_values)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(x_values, y_values, color='b', label='$y(x)$')\n",
    "plt.scatter(X, Y, color='r', s=16, label=r'$r(x) + \\varepsilon$ (train)')\n",
    "plt.legend(loc='upper right');\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex_kernel_pdf_sin_first'></a>\n",
    "### Первичная оценка функции регрессии<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = np.argsort(X)\n",
    "X = X[permutation]\n",
    "Y = Y[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = 1.\n",
    "nw_regressor = NadarayaWatsonRegressor(GaussianKernel(), bandwidth)\n",
    "nw_regressor.fit(X, Y)\n",
    "Y_pred = nw_regressor.predict(X)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(X, Y_pred, color='b', label='$\\hat{{r}}(x)$ by NW with $h = {}$'.format(bandwidth), zorder=2)\n",
    "plt.plot(X, Y, color='g', marker='o', markersize=4, linestyle='none', label=r'$r(x) + \\varepsilon$ (train)',\n",
    "         zorder=2)\n",
    "plt.legend()\n",
    "plt.xlabel('$x$'); plt.ylabel('$y$');\n",
    "plt.xlim([X.min() - 0.1, X.max() + 0.1])\n",
    "plt.ylim([Y.min() - 1, Y.max() + 1])\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex_kernel_pdf_sin_opt'></a>\n",
    "### Оптимальная ширина ядра<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth_range = np.linspace(0.07, 0.9, 100)\n",
    "risk_values = []\n",
    "for h in bandwidth_range:\n",
    "    risk_values.append(nw_regression_estimated_risk(X, Y, h))\n",
    "risk_values = np.array(risk_values)\n",
    "plt.plot(bandwidth_range, risk_values, color='b', zorder=2)\n",
    "plt.xlabel('$h$'); plt.ylabel(r'$\\hat{J}(h)$');\n",
    "plt.title(r'To finding minimum of $\\hat{J}(h)$');\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_bandwidth = bandwidth_range[np.argmin(risk_values)]\n",
    "print(\"Minimal value of J(h) reached at point h = {}, J(h) = {}\".format(opt_bandwidth, np.min(risk_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = opt_bandwidth\n",
    "nw_regressor = NadarayaWatsonRegressor(GaussianKernel(), bandwidth)\n",
    "nw_regressor.fit(X, Y)\n",
    "Y_pred = nw_regressor.predict(X)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(X, Y_pred, color='b',label=r'$\\hat{{r}}(x)$ by NW with $h = {:.2}$'.format(bandwidth))\n",
    "plt.plot(X, Y, color='g', marker='o', markersize=4, linestyle='none', label=r'$r(x) + \\varepsilon$ (train)')\n",
    "plt.legend()\n",
    "plt.xlabel('$x$'); plt.ylabel('$y$');\n",
    "plt.title(r'$\\hat{r}(x)$ for optimal value of $h$')\n",
    "plt.xlim([X.min() - 0.1, X.max() + 0.1])\n",
    "plt.ylim([Y.min() - 5, Y.max() + 5])\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex_kernel_pdf_sin_conf'></a>\n",
    "### Построение доверительной трубки<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "h = opt_bandwidth # Optimal value\n",
    "n = len(Y)\n",
    "a = np.min(X)\n",
    "b = np.max(X)\n",
    "m = (b - a) / (3 * h)\n",
    "q = stats.norm.ppf((1 + (1 - alpha) ** (1 / m)) / 2)\n",
    "print('q =', q)\n",
    "sigma = np.sqrt(np.sum((Y[1:] - Y[:-1]) ** 2)  / (2 * (n - 1)))\n",
    "print('sigma =', sigma)\n",
    "\n",
    "x_values = np.linspace(a, b, 400)\n",
    "y_values = np.sin(x_values)\n",
    "\n",
    "kernel = GaussianKernel()\n",
    "K = kernel((x_values[:, None] - X[None, :]) / h)\n",
    "W = K / K.sum(axis=1)[:, None]\n",
    "se = sigma * np.sqrt(np.sum(W ** 2, axis=1))\n",
    "\n",
    "nw_regressor = NadarayaWatsonRegressor(kernel, h)\n",
    "nw_regressor.fit(X, Y)\n",
    "Y_pred = nw_regressor.predict(x_values)\n",
    "\n",
    "lower = Y_pred - q * se\n",
    "upper = Y_pred + q * se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.fill_between(x_values, lower, upper, color='b', alpha=0.2, zorder=1)\n",
    "plt.plot(x_values, y_values, color='b', linestyle='-', linewidth=2, label=r'$r(x)$')\n",
    "plt.plot(x_values, lower, color='k', linewidth=1, linestyle='--')\n",
    "plt.plot(x_values, upper, color='k', linewidth=1, linestyle='--')\n",
    "plt.plot(x_values, Y_pred, color='r', linestyle='--', label=r'$\\hat{r}(x)$')\n",
    "plt.plot(X, Y, color='g', marker='o', markersize=4, linestyle='none', label=r'$r(x) + \\varepsilon$ (train)')\n",
    "plt.xlabel('$x$'); plt.ylabel('$y$'); plt.title('{:.1f}\\% confidence band'.format(100 * (1 - alpha)))\n",
    "plt.xlim([X.min() - 0.1, X.max() + 0.1])\n",
    "plt.ylim([Y.min() - 3, Y.max() + 3])\n",
    "plt.legend()\n",
    "plt.grid(which='both', alpha=0.5, linestyle='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='nonparam_regr_example_lidar'></a>\n",
    "## LiDAR<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(DATASETS_DIR, 'lidar.dat.txt'), sep='\\s+')\n",
    "data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(data['range'], data['logratio'], color='b', alpha=0.5)\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('frequency')\n",
    "plt.ylabel('logratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регрессия Надарая-Ватсона<sup>[toc](#_toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['range'].to_numpy()\n",
    "Y = data['logratio'].to_numpy()\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "bandwidth = 5\n",
    "nw_regressor = NadarayaWatsonRegressor(GaussianKernel(), bandwidth)\n",
    "indices = np.argsort(X)\n",
    "X = X[indices]\n",
    "Y = Y[indices]\n",
    "nw_regressor.fit(X, Y)\n",
    "Y_pred = nw_regressor.predict(X)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(X, Y_pred, color='b', label='$\\hat{{r}}(x)$ by NW with $h = {}$'.format(bandwidth), zorder=2)\n",
    "plt.plot(X, Y, color='g', marker='o', markersize=4, linestyle='none', label=r'$r(x) + \\varepsilon$ (train)',\n",
    "         zorder=2)\n",
    "plt.plot(x_values, np.sin(x_values), color='r', label='$r(x)$')\n",
    "plt.legend()\n",
    "plt.xlabel('$x$');\n",
    "plt.ylabel('$y$');\n",
    "\n",
    "x_span = X.max() - X.min()\n",
    "plt.xlim([X.min() - 0.1 * x_span, X.max() + 0.1 * x_span])\n",
    "\n",
    "y_span = Y.max() - Y.min()\n",
    "plt.ylim([Y.min() - 0.1 * y_span, Y.max() + 0.1 * y_span])\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
