{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\PI}{3.141592654}\n",
    "\\newcommand{\\Sum}{\\sum\\limits}\n",
    "\\newcommand{\\Int}{\\int\\limits}\n",
    "\\newcommand{\\Lim}{\\lim\\limits}\n",
    "\\newcommand{\\Intf}{\\int\\limits_{-\\infty}^{+\\infty}}\n",
    "\\newcommand{\\Prod}{\\prod\\limits}\n",
    "\\newcommand{\\Max}{\\max\\limits}\n",
    "\\newcommand{\\Min}{\\min\\limits}\n",
    "\\newcommand{\\Var}{\\mathbb{V}}\n",
    "\\newcommand{\\Exp}{\\mathbb{E}}\n",
    "\\newcommand{\\argmax}{\\arg\\max}\n",
    "\\newcommand{\\argmin}{\\arg\\min}\n",
    "\\newcommand{\\Cov}{\\text{Cov}}\n",
    "\\newcommand{\\Loss}{\\mathcal{L}}\n",
    "\\newcommand{\\LogLike}{\\mathcal{L}}\n",
    "\\newcommand{\\Like}{\\ell}\n",
    "\\newcommand{\\Risk}{\\mathcal{R}}\n",
    "\\newcommand{\\makebold}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\mean}[1]{\\overline{#1}}\n",
    "\\newcommand{\\avg}[1]{\\left\\langle #1 \\right\\rangle}\n",
    "\\newcommand{\\eps}{\\varepsilon}\n",
    "\\renewcommand{\\epsilon}{\\varepsilon}\n",
    "\\newcommand{\\partfrac}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\ttt}[1]{\\texttt{#1}}\n",
    "\\newcommand{\\term}[1]{\\textbf{#1}}\n",
    "\\newcommand{\\lp}{\\left(}\n",
    "\\newcommand{\\rp}{\\right)}\n",
    "\\newcommand{\\lf}{\\left\\{}\n",
    "\\newcommand{\\rf}{\\right\\}}\n",
    "\\newcommand{\\ls}{\\left[}\n",
    "\\newcommand{\\rs}{\\right]}\n",
    "\\newcommand{\\lv}{\\left|}\n",
    "\\newcommand{\\rv}{\\right|}\n",
    "\\newcommand{\\RR}{\\mathbb{R}}\n",
    "\\newcommand{\\NN}{\\mathbb{N}}\n",
    "\\newcommand{\\ZZ}{\\mathbb{Z}}\n",
    "\\newcommand{\\Ecdf}[1]{\\hat{F}_n(#1)}\n",
    "\\newcommand{\\OPT}{\\ensuremath{\\mathrm{OPT}}}\n",
    "\\newcommand{\\opt}{\\ensuremath{\\mathrm{opt}}}\n",
    "\\newcommand{\\boot}{\\ensuremath{\\mathrm{boot}}}\n",
    "\\newcommand{\\bias}{\\ensuremath{\\mathrm{bias}}}\n",
    "\\newcommand{\\se}{\\ensuremath{\\mathrm{se}}}\n",
    "\\newcommand{\\MSE}{\\ensuremath{\\mathrm{MSE}}}\n",
    "\\newcommand{\\qm}{\\ensuremath{\\mathrm{qm}}}\n",
    "\\newcommand{\\as}{\\ensuremath{\\mathrm{as}}}\n",
    "\\newcommand{\\trace}{\\mathrm{tr}}\n",
    "\\newcommand{\\const}{\\mathrm{const}}\n",
    "\\newcommand{\\sign}{\\mathrm{sign}}\n",
    "\\newcommand{\\tr}{\\mathrm{tr}}\n",
    "\\newcommand{\\new}{\\mathrm{new}}\n",
    "\\newcommand{\\old}{\\mathrm{old}}\n",
    "\\newcommand{\\diag}{\\mathrm{diag}}\n",
    "\\newcommand{\\rank}{\\mathrm{rg}}\n",
    "\\newcommand{\\ML}{\\mathrm{ML}}\n",
    "\\newcommand{\\MP}{\\mathrm{MP}}\n",
    "\\newcommand{\\KL}{\\mathrm{KL}}\n",
    "\\newcommand{\\NV}{\\mathrm{NV}}\n",
    "\\newcommand{\\esttheta}{\\hat{\\theta}}\n",
    "\\newcommand{\\estlambda}{\\hat{\\lambda}}\n",
    "\\newcommand{\\estmu}{\\hat{\\mu}}\n",
    "\\newcommand{\\estsigma}{\\hat{\\sigma}}\n",
    "\\newcommand{\\estalpha}{\\hat{\\alpha}}\n",
    "\\newcommand{\\estbeta}{\\hat{\\beta}}\n",
    "\\newcommand{\\estxi}{\\hat{\\xi}}\n",
    "\\newcommand{\\esttau}{\\hat{\\tau}}\n",
    "\\newcommand{\\estpsi}{\\hat{\\psi}}\n",
    "\\newcommand{\\esta}{\\hat{a}}\n",
    "\\newcommand{\\estb}{\\hat{b}}\n",
    "\\newcommand{\\estc}{\\hat{c}}\n",
    "\\newcommand{\\estd}{\\hat{d}}\n",
    "\\newcommand{\\estf}{\\hat{f}}\n",
    "\\newcommand{\\estp}{\\hat{p}}\n",
    "\\newcommand{\\esty}{\\hat{y}}\n",
    "\\newcommand{\\estT}{\\hat{T}}\n",
    "\\newcommand{\\estR}{\\hat{R}}\n",
    "\\newcommand{\\estF}{\\hat{F}}\n",
    "\\newcommand{\\estC}{\\hat{C}}\n",
    "\\newcommand{\\estS}{\\hat{S}}\n",
    "\\newcommand{\\estY}{\\hat{Y}}\n",
    "\\newcommand{\\estVar}{\\hat{\\Var}}\n",
    "\\newcommand{\\estExp}{\\hat{\\Exp}}\n",
    "\\newcommand{\\estSe}{\\hat{\\se}}\n",
    "\\newcommand{\\ecdf}{\\hat{F}}\n",
    "\\newcommand{\\hata}{\\hat{a}}\n",
    "\\newcommand{\\hatb}{\\hat{b}}\n",
    "\\newcommand{\\hatc}{\\hat{c}}\n",
    "\\newcommand{\\hatd}{\\hat{d}}\n",
    "\\newcommand{\\hatf}{\\hat{f}}\n",
    "\\newcommand{\\hatg}{\\hat{g}}\n",
    "\\newcommand{\\hatk}{\\hat{k}}\n",
    "\\newcommand{\\hatp}{\\hat{p}}\n",
    "\\newcommand{\\hatr}{\\hat{r}}\n",
    "\\newcommand{\\hatt}{\\hat{t}}\n",
    "\\newcommand{\\haty}{\\hat{y}}\n",
    "\\newcommand{\\hatC}{\\hat{C}}\n",
    "\\newcommand{\\hatF}{\\hat{F}}\n",
    "\\newcommand{\\hatJ}{\\hat{J}}\n",
    "\\newcommand{\\hatK}{\\hat{K}}\n",
    "\\newcommand{\\hatY}{\\hat{Y}}\n",
    "\\newcommand{\\hateps}{\\hat{\\eps}}\n",
    "\\newcommand{\\hatalpha}{\\hat{\\alpha}}\n",
    "\\newcommand{\\hatbeta}{\\hat{\\beta}}\n",
    "\\newcommand{\\hatpsi}{\\hat{\\psi}}\n",
    "\\newcommand{\\hatlambda}{\\hat{\\lambda}}\n",
    "\\newcommand{\\hattheta}{\\hat{\\theta}}\n",
    "\\newcommand{\\hatsigma}{\\hat{\\sigma}}\n",
    "\\newcommand{\\hatboldk}{\\hat{\\boldk}}\n",
    "\\newcommand{\\hatSe}{\\hat{\\se}}\n",
    "\\newcommand{\\hatExp}{\\hat{\\Exp}}\n",
    "\\newcommand{\\hatVar}{\\hat{\\Var}}\n",
    "\\newcommand{\\tilx}{\\tilde{x}}\n",
    "\\newcommand{\\tily}{\\tilde{y}}\n",
    "\\newcommand{\\tilX}{\\tilde{X}}\n",
    "\\newcommand{\\tilY}{\\tilde{Y}}\n",
    "\\newcommand{\\tilK}{\\tilde{K}}\n",
    "\\newcommand{\\tiltau}{\\tilde{\\tau}}\n",
    "\\newcommand{\\tiltheta}{\\tilde{\\theta}}\n",
    "\\newcommand{\\tillambda}{\\tilde{\\lambda}}\n",
    "\\newcommand{\\tilsigma}{\\tilde{\\sigma}}\n",
    "\\newcommand{\\tilpsi}{\\tilde{\\psi}}\n",
    "\\newcommand{\\mlexi}{\\xi_{MLE}}\n",
    "\\newcommand{\\mletheta}{\\theta_{MLE}}\n",
    "\\newcommand{\\mlelambda}{\\lambda_{MLE}}\n",
    "\\newcommand{\\mlesigma}{\\sigma_{MLE}}\n",
    "\\newcommand{\\mlepsi}{\\psi_{MLE}}\n",
    "\\newcommand{\\mmxi}{\\xi_{MM}}\n",
    "\\newcommand{\\mmtheta}{\\theta_{MM}}\n",
    "\\newcommand{\\mmlambda}{\\lambda_{MM}}\n",
    "\\newcommand{\\mmsigma}{\\sigma_{MM}}\n",
    "\\newcommand{\\mmpsi}{\\psi_{MM}}\n",
    "\\newcommand{\\mmalpha}{\\alpha_{MM}}\n",
    "\\newcommand{\\mmbeta}{\\beta_{MM}}\n",
    "\\newcommand{\\Poisson}{\\mathrm{Poisson}}\n",
    "\\newcommand{\\Uniform}{\\mathrm{Uniform}}\n",
    "\\newcommand{\\Binomial}{\\mathrm{Binomial}}\n",
    "\\newcommand{\\Gammap}{\\mathrm{Gamma}}\n",
    "\\newcommand{\\Normal}{\\mathcal{N}}\n",
    "\\newcommand{\\LogN}{\\mathrm{LogN}}\n",
    "\\newcommand{\\Exponential}{\\mathrm{Exp}}\n",
    "\\newcommand{\\Erlang}{\\mathrm{Erlang}}\n",
    "\\newcommand{\\Cauchy}{C}\n",
    "\\newcommand{\\Dir}{\\mathrm{Dir}}\n",
    "\\newcommand{\\Beta}{\\mathrm{Beta}}\n",
    "\\newcommand{\\Family}{\\mathfrak{F}}\n",
    "\\newcommand{\\RejectRegion}{R}\n",
    "\\newcommand{\\pvalue}{\\text{p-value}}\n",
    "\\newcommand{\\llr}{\\ell}\n",
    "\\newcommand{\\Llr}{\\mathcal{L}}\n",
    "\\newcommand{\\RRS}{\\mathrm{RSS}}\n",
    "\\newcommand{\\redtext}[1]{\\textcolor{red}{#1}}\n",
    "\\newcommand{\\addtask}[1]{\\hyperref[#1]{\\redtext{Задача~\\ref*{#1}}}}\n",
    "\\newcommand{\\solution}{\\redtext{\\textbf{Решение.}}}\n",
    "\\newcommand{\\ignore}[1]{}\n",
    "\\newcommand{\\NumOfSamples}{\\mathcal{N}}\n",
    "\\newcommand{\\NumOfDims}{\\mathcal{D}}\n",
    "\\newcommand{\\NumOfHidden}{\\mathcal{H}}\n",
    "\\newcommand{\\NumOfClasses}{\\mathcal{K}}\n",
    "\\newcommand{\\NumOfChannels}{\\mathcal{C}}\n",
    "\\newcommand{\\NumOfFilters}{\\mathcal{F}}\n",
    "\\newcommand{\\HiddenSize}{\\mathcal{H}}\n",
    "\\newcommand{\\bolda}{\\boldsymbol{a}}\n",
    "\\newcommand{\\boldb}{\\boldsymbol{b}}\n",
    "\\newcommand{\\bolde}{\\boldsymbol{e}}\n",
    "\\newcommand{\\boldf}{\\boldsymbol{f}}\n",
    "\\newcommand{\\boldg}{\\boldsymbol{g}}\n",
    "\\newcommand{\\boldh}{\\boldsymbol{h}}\n",
    "\\newcommand{\\boldm}{\\boldsymbol{m}}\n",
    "\\newcommand{\\boldk}{\\boldsymbol{k}}\n",
    "\\newcommand{\\bolds}{\\boldsymbol{s}}\n",
    "\\newcommand{\\boldt}{\\boldsymbol{t}}\n",
    "\\newcommand{\\boldp}{\\boldsymbol{p}}\n",
    "\\newcommand{\\boldw}{\\boldsymbol{w}}\n",
    "\\newcommand{\\boldx}{\\boldsymbol{x}}\n",
    "\\newcommand{\\boldy}{\\boldsymbol{y}}\n",
    "\\newcommand{\\boldu}{\\boldsymbol{u}}\n",
    "\\newcommand{\\boldv}{\\boldsymbol{v}}\n",
    "\\newcommand{\\boldz}{\\boldsymbol{z}}\n",
    "\\newcommand{\\boldA}{\\boldsymbol{A}}\n",
    "\\newcommand{\\boldB}{\\boldsymbol{B}}\n",
    "\\newcommand{\\boldC}{\\boldsymbol{C}}\n",
    "\\newcommand{\\boldD}{\\boldsymbol{D}}\n",
    "\\newcommand{\\boldE}{\\boldsymbol{E}}\n",
    "\\newcommand{\\boldF}{\\boldsymbol{F}}\n",
    "\\newcommand{\\boldH}{\\boldsymbol{H}}\n",
    "\\newcommand{\\boldJ}{\\boldsymbol{J}}\n",
    "\\newcommand{\\boldK}{\\boldsymbol{K}}\n",
    "\\newcommand{\\boldM}{\\boldsymbol{M}}\n",
    "\\newcommand{\\boldI}{\\boldsymbol{I}}\n",
    "\\newcommand{\\boldP}{\\boldsymbol{P}}\n",
    "\\newcommand{\\boldR}{\\boldsymbol{R}}\n",
    "\\newcommand{\\boldS}{\\boldsymbol{S}}\n",
    "\\newcommand{\\boldT}{\\boldsymbol{T}}\n",
    "\\newcommand{\\boldO}{\\boldsymbol{O}}\n",
    "\\newcommand{\\boldU}{\\boldsymbol{U}}\n",
    "\\newcommand{\\boldV}{\\boldsymbol{V}}\n",
    "\\newcommand{\\boldW}{\\boldsymbol{W}}\n",
    "\\newcommand{\\boldX}{\\boldsymbol{X}}\n",
    "\\newcommand{\\boldY}{\\boldsymbol{Y}}\n",
    "\\newcommand{\\boldZ}{\\boldsymbol{Z}}\n",
    "\\newcommand{\\boldXY}{\\boldsymbol{XY}}\n",
    "\\newcommand{\\boldzero}{\\boldsymbol{0}}\n",
    "\\newcommand{\\boldalpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\boldbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\boldtheta}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\boldmu}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\boldxi}{\\boldsymbol{\\xi}}\n",
    "\\newcommand{\\boldeta}{\\boldsymbol{\\eta}}\n",
    "\\newcommand{\\boldpi}{\\boldsymbol{\\pi}}\n",
    "\\newcommand{\\boldsigma}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\boldphi}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\boldlambda}{\\boldsymbol{\\lambda}}\n",
    "\\newcommand{\\boldeps}{\\boldsymbol{\\eps}}\n",
    "\\newcommand{\\boldPhi}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\boldLambda}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\hatboldK}{\\hat{\\boldK}}\n",
    "\\newcommand{\\hatboldC}{\\hat{\\boldC}}\n",
    "\\newcommand{\\hatboldy}{\\hat{\\boldy}}\n",
    "\\newcommand{\\hatboldalpha}{\\hat{\\boldsymbol{\\alpha}}}\n",
    "\\newcommand{\\hatboldbeta}{\\hat{\\boldsymbol{\\beta}}}\n",
    "\\newcommand{\\hatboldtheta}{\\hat{\\boldsymbol{\\theta}}}\n",
    "\\newcommand{\\hatboldeps}{\\hat{\\boldsymbol{\\eps}}}\n",
    "\\newcommand{\\tilboldbeta}{\\tilde{\\boldbeta}}\n",
    "\\newcommand{\\xs}[1]{\\boldx^{(#1)}}\n",
    "\\newcommand{\\ys}[1]{\\boldy^{(#1)}}\n",
    "\\newcommand{\\zs}[1]{\\boldz^{(#1)}}\n",
    "\\newcommand{\\Xs}[1]{\\boldX^{(#1)}}\n",
    "\\newcommand{\\Ys}[1]{\\boldY^{(#1)}}\n",
    "\\newcommand{\\Zs}[1]{\\boldZ^{(#1)}}\n",
    "\\newcommand{\\hatSigma}{\\hat{\\Sigma}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регрессия на основе гауссовских процессов\n",
    "* Документация по гауссовским процессам в библиотеке scikit-learn: http://scikit-learn.org/stable/modules/gaussian_process.html\n",
    "* Материалы школы DeepBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/siri3us/ASML/blob/master/notebooks/seminar_10_gpr.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import copy\n",
    "\n",
    "from scipy import stats\n",
    "from itertools import product\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from itertools import combinations, product\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import pickle as pkl\n",
    "\n",
    "# sklearn\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.kernel_ridge import KernelRidge, pairwise_kernels\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "from matplotlib.colors import colorConverter\n",
    "%matplotlib inline\n",
    "\n",
    "titlesize = 24\n",
    "labelsize = 22\n",
    "legendsize = 22\n",
    "xticksize = 18\n",
    "yticksize = xticksize\n",
    "\n",
    "\n",
    "matplotlib.rcParams['legend.markerscale'] = 1.5     # the relative size of legend markers vs. original\n",
    "matplotlib.rcParams['legend.handletextpad'] = 0.5\n",
    "matplotlib.rcParams['legend.labelspacing'] = 0.4    # the vertical space between the legend entries in fraction of fontsize\n",
    "matplotlib.rcParams['legend.borderpad'] = 0.5       # border whitespace in fontsize units\n",
    "matplotlib.rcParams['font.size'] = 12\n",
    "matplotlib.rcParams['font.family'] = 'serif'\n",
    "matplotlib.rcParams['font.serif'] = 'Times New Roman'\n",
    "matplotlib.rcParams['axes.labelsize'] = labelsize\n",
    "matplotlib.rcParams['axes.titlesize'] = titlesize\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 8)    \n",
    "\n",
    "matplotlib.rc('xtick', labelsize=xticksize)\n",
    "matplotlib.rc('ytick', labelsize=yticksize)\n",
    "matplotlib.rc('legend', fontsize=legendsize)\n",
    "\n",
    "matplotlib.rc('font', **{'family':'serif'})\n",
    "matplotlib.rc('text', usetex=True)\n",
    "matplotlib.rc('text.latex', unicode=True)\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage[english]{babel}')\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage{amsmath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_NOT_SAVE_IMAGES = True\n",
    "GIT_DIR = 'ASML-book'\n",
    "if not DO_NOT_SAVE_IMAGES:\n",
    "    ASML_DIR = os.getcwd().split('ASML')[0] + GIT_DIR\n",
    "    PICS_DIR = os.path.join(ASML_DIR, 'book', 'pics', 'gpr')\n",
    "    print(ASML_DIR)\n",
    "    print(PICS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "# Содержание\n",
    "* <a href='#gpr_theory'>Регрессия на основе гауссовских процессов: теория</a>\n",
    "    * [Гауссовский случайный процесс](#gpr_theory_gpr)\n",
    "    * [Апостериорная оценка значений процесса в случае отсутствия шума](#gpr_theory_noise_free)\n",
    "    * [Апостериорная оценка значений процесса в случае наличия шума](#gpr_theory_noise)\n",
    "    * [Выбор параметров ковариационной функции (ядра)](#gpr_theory_mle)\n",
    "* [Регрессия на основе гауссовских процессов: практика](#gpr_practice)\n",
    "    * [Гауссовские процессы в библиотеке sklearn](#gpr_sklearn)\n",
    "        * [GaussianProcessRegressor](#doc_gpr)\n",
    "        * [Доступные ядра в библиотеке sklearn](#gpr_sklearn_kernels)\n",
    "    * [Устойчивость модели гауссовой регрессии](#gpr_sustainability)\n",
    "    * [Восстановление зависимостей с помощью гауссовской регрессии](#gpr_example1)\n",
    "    * [Automatic relevance determination (ARD)](#gpr_ard)\n",
    "    * [Визуализация правдоподобия](#gpr_mle_visualization)\n",
    "* <a href='#automl'>AutoML</a>\n",
    "    * [Linear SVM](#automl_linear_svm)\n",
    "    * [TASK. Kernel SVM](#automl_kernel_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpr_theory'></a>\n",
    "# Регрессия на основе гауссовских процессов: теория<sup><a href='#toc'>toc</a></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpr_theory_gpr'></a>\n",
    "### Гауссовский случайный процесс<sup>[toc](#toc)</sup>\n",
    "\n",
    "Рассмотрим случайный процесс $f(\\boldx,\\omega)$, где $\\boldx \\in \\RR^d$ &mdash; индексирующий параметр,  $\\omega \\in \\Omega$ &mdash; элементарный исход. Далее будем опускать $\\omega$ и писать просто $f(\\boldx)$. \n",
    "\n",
    "Рассмотрим совокупность значений $f(\\boldx)$ в точках $\\boldX = \\{\\boldx_1, \\dots, \\boldx_n\\}$, т.е. **сечение процесса по $\\boldX$** или **проекцию процесса на $\\boldX$**. Далее будем обозначать\n",
    "\\begin{gather*}\n",
    "\\boldf(\\boldX) \\triangleq (f(\\boldx_1), \\dots, f(\\boldx_n))^T.\n",
    "\\end{gather*}\n",
    "**Случайный процесс $f(\\boldx)$ называется гауссовским, если все его конечномерные распределения являются гауссовскими**\n",
    "\\begin{gather*}\n",
    "p(\\boldf(\\boldX)) = p(f(\\boldx_1), \\dots, f(\\boldx_n)) = \\Normal(\\boldf(\\boldX)|\\boldmu_{\\boldX},\\Sigma_{\\boldX}),\n",
    "\\end{gather*}\n",
    "где $\\boldmu_{\\boldX}$ &mdash; вектор средних значений и $\\Sigma_{\\boldX}$ &mdash; матрица ковариаций в общем случае зависящие от сечения $\\boldX = \\{\\boldx_1, \\dots, \\boldx_n\\}$:\n",
    "\\begin{gather*}\n",
    "\\Sigma_{\\boldX} = \n",
    "\\begin{pmatrix} \n",
    "K(\\boldx_1, \\boldx_1) &\\dots  &K(\\boldx_1, \\boldx_n) \\\\\n",
    "\\vdots                &\\ddots &\\vdots    \\\\\n",
    "K(\\boldx_n, \\boldx_1) &\\dots  &K(\\boldx_n, \\boldx_n)  \n",
    "\\end{pmatrix} \\in \\RR^{n \\times n}.\n",
    "\\end{gather*}\n",
    "\n",
    "У **стационарного в узком смысле** гауссовского процесса вектор средних значений &mdash; константа, а ковариация между значениями случайного процесса $f(\\boldx)$ в точках $\\boldx_i$ и $\\boldx_j$ зависит только от расстояния между этими точками:\n",
    "\\begin{gather*}\n",
    "\\boldmu_{\\boldX} = \\const,\\\\\n",
    "K(\\boldx_i, \\boldx_j) \\triangleq \\Cov(f(\\boldx_i), f(\\boldx_j)) = K(\\rho(\\boldx_i, \\boldx_j)).\n",
    "\\end{gather*}\n",
    "Далее будем писать просто $K(\\boldx_i, \\boldx_j)$. Заметим, что при этом априорная дисперсия значения процесса в некоторой точке $\\boldx$ одинакова для всех точек и равна $K(0)$:\n",
    "\\begin{gather*}\n",
    "\\Var \\lp f(\\boldx) \\rp = \\Cov(f(\\boldx), f(\\boldx)) = K(\\boldx, \\boldx) = K(0).\n",
    "\\end{gather*}\n",
    "В силу неравенства Коши-Буняковского для стационарного в узком смысле процесса получаем:\n",
    "$$\n",
    "K(\\boldx',\\boldx'') = K(\\rho(\\boldx',\\boldx'')) = \\sqrt{K(\\rho(\\boldx', \\boldx')) K(\\rho(\\boldx'',\\boldx''))} = K(0). \n",
    "$$\n",
    "\n",
    "На самом деле требование станционарности не является необходимым при работе с гауссовскими процессами, о чем мы убедимся далее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpr_theory_noise_free'></a>\n",
    "### Апостериорная оценка значений процесса в случае отсутствия шума<sup>[toc](#toc)</sup>\n",
    "\n",
    "Требуется оценить значения процесса в точках $\\boldY = \\{\\boldy_1, \\dots, \\boldy_m\\}$. Введем следующие обозначения:\n",
    "\\begin{gather*}\n",
    "\\boldf(\\boldY) \\triangleq (f(\\boldy_1), \\dots, f(\\boldy_m))^T,\\\\\n",
    "\\boldf(\\boldX\\boldY) \\triangleq (f(\\boldx_1), \\dots, f(\\boldx_n), f(\\boldy_1), \\dots, f(\\boldy_m))^T.\n",
    "\\end{gather*}\n",
    "\n",
    "По определению условной вероятности получаем\n",
    "\\begin{gather*}\n",
    "p(\\boldf(\\boldY)|\\boldf(\\boldX)) = \\frac{p(\\boldf(\\boldY),\\boldf(\\boldX))}{p(\\boldX)}.\n",
    "\\end{gather*}\n",
    "Матрица ковариаций для вектора $\\boldf(\\boldX\\boldY)$ имеет вид:\n",
    "\\begin{gather*}\n",
    "\\Sigma_{\\boldX\\boldY} = \\begin{pmatrix}\n",
    "\\Sigma_{\\boldX} &\\boldK_{\\boldX,\\boldY} \\\\\n",
    "\\boldK_{\\boldX, \\boldY} &\\Sigma_{\\boldY}\n",
    "\\end{pmatrix},\n",
    "\\end{gather*}\n",
    "где \n",
    "\\begin{gather*}\n",
    "\\Sigma_{\\boldX} =\n",
    "\\begin{pmatrix} \n",
    "K(\\boldx_1, \\boldx_1) &\\dots  &K(\\boldx_1, \\boldx_n) \\\\\n",
    "\\vdots                &\\ddots &\\vdots    \\\\\n",
    "K(\\boldx_n, \\boldx_1) &\\dots  &K(\\boldx_n, \\boldx_n)  \n",
    "\\end{pmatrix} \\in \\RR^{n \\times n},\\quad\n",
    "\\Sigma_{\\boldY} =\n",
    "\\begin{pmatrix} \n",
    "K(\\boldy_1, \\boldy_1) &\\dots  &K(\\boldy_1, \\boldy_m) \\\\\n",
    "\\vdots                &\\ddots &\\vdots    \\\\\n",
    "K(\\boldy_m, \\boldy_1) &\\dots  &K(\\boldy_m, \\boldy_m)  \n",
    "\\end{pmatrix} \\in \\RR^{m \\times m},\\quad\n",
    "\\boldK_{\\boldX,\\boldY} = \n",
    "\\begin{pmatrix} \n",
    "K(\\boldx_1, \\boldy_1) &\\dots  &K(\\boldx_1, \\boldy_m) \\\\\n",
    "\\vdots                &\\ddots &\\vdots    \\\\\n",
    "K(\\boldx_n, \\boldy_1) &\\dots  &K(\\boldx_n, \\boldy_m)  \n",
    "\\end{pmatrix} \\in \\RR^{n \\times m}.\n",
    "\\end{gather*}\n",
    "\n",
    "Можно показать, что \n",
    "\\begin{align*}\n",
    "&\\boxed{p(\\boldf(\\boldY)|\\boldf(\\boldX)) = \\frac{p(\\boldf(\\boldY),\\boldf(\\boldX))}{p(\\boldX)} = \\Normal(\\boldf(\\boldY)|\\boldmu_{\\boldY|\\boldX}, \\Sigma_{\\boldY|\\boldX})},\\\\\n",
    "&\\boxed{\\boldmu_{\\boldY|\\boldX} = \\boldmu_{\\boldY} + \\boldK^T \\Sigma_{\\boldX}^{-1} (\\boldf(\\boldX) - \\boldmu_{\\boldX})},\\\\\n",
    "&\\boxed{\\Sigma_{\\boldY|\\boldX} = \\Sigma_{\\boldY} - \\boldK^T\\Sigma_{\\boldX}^{-1}\\boldK}.\n",
    "\\end{align*}\n",
    "\n",
    "Если мы предсказываем значение в одной точке, т.е. $\\boldY = \\{\\boldy\\}$, тогда получим, что\n",
    "\\begin{align*}\n",
    "&\\boxed{p(f(\\boldy)|\\boldf(\\boldX)) = \\frac{p(f(\\boldy),\\boldf(\\boldX))}{p(\\boldX)} = \\Normal(\\boldf(\\boldy)|\\mu_{\\boldy|\\boldX}, \\sigma_{\\boldy|\\boldX}^2)},\\\\\n",
    "&\\boxed{\\mu_{\\boldy|\\boldX} = \\boldmu_{\\boldy} + \\boldk^T(\\boldy) \\Sigma_{\\boldX}^{-1} (\\boldf(\\boldX) - \\boldmu_{\\boldX})},\\\\\n",
    "&\\boxed{\\sigma_{\\boldy|\\boldX}^2 = K(0) - \\boldk^T(\\boldy)\\Sigma_{\\boldX}^{-1}\\boldk(\\boldy)}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpr_theory_noise'></a>\n",
    "### Апостериорная оценка значений процесса в случае наличия шума<sup>[toc](#toc)</sup>\n",
    "\n",
    "Дана выборка $(\\boldX, \\boldt) = \\{ (\\boldx_i, t_i) \\}_{i = 1}^n$, $\\boldx_i \\in \\RR^d$, $t_i \\in \\RR$. Вводится предположение о том, что наблюдаемые значения $\\boldt$ &mdash; это зашумленная реализация $t(\\boldX) = \\lf t(\\boldx_1), \\dots, t(\\boldx_n) \\rf$ некоторого гауссовского случайного процесса $f(\\boldx)$:\n",
    "$$\n",
    "t(\\boldx) = \\boldf(x) + \\eps, \\qquad \\eps \\sim \\Normal(0, \\sigma^2).\n",
    "$$\n",
    "\n",
    "Заметим, что $t(\\boldx)$ также является гауссовским случайным процессом, потому как любое конечномерное распределение $p(\\boldt(\\boldX))$ является нормальным. Функция ковариации этого процесса связана с функцией ковариации $K(\\cdot)$ процесса $f(\\boldx)$ следующим образом:\n",
    "\\begin{gather*}\n",
    "\\hat{K}(\\boldx_i, \\boldx_j) = K(\\boldx_i, \\boldx_j) + \\sigma^2 \\delta_{\\boldx_i, \\boldx_j} =\n",
    "\\begin{cases}\n",
    "K(\\boldx_i, \\boldx_j), &\\boldx_i \\neq \\boldx_j,\\\\\n",
    "K(0) + \\sigma^2, &\\boldx_i = \\boldx_j.\n",
    "\\end{cases}\n",
    "\\end{gather*}\n",
    "Поэтому\n",
    "\\begin{align*}\n",
    "&\\boxed{p(\\boldf(\\boldY)|\\boldt(\\boldX)) = \\Normal(\\boldf(\\boldY)|\\boldmu_{\\boldY|\\boldX}, \\Sigma_{\\boldY|\\boldX})},\\\\\n",
    "&\\boxed{\\boldmu_{\\boldY|\\boldX} = \\boldmu_{\\boldY} + \\hat{\\boldK}^T \\hatSigma_{\\boldX}^{-1}(\\boldf(\\boldX) - \\boldmu_{\\boldX})},\\\\\n",
    "&\\boxed{\\Sigma_{\\boldY|\\boldX} = \\hatSigma_{\\boldY} - \\hat{\\boldK}^T\\hatSigma_{\\boldX}^{-1}\\hat{\\boldK}},\\\\\n",
    "&\\boxed{\\hatSigma_{\\boldX} = \\Sigma_{\\boldX} + \\sigma^2 I, \\qquad \\hatSigma_{\\boldY} = \\Sigma_{\\boldY} + \\sigma^2 }.\n",
    "\\end{align*}\n",
    "\n",
    "В случае предсказания в одной точке $\\boldy$\n",
    "\\begin{align*}\n",
    "&\\boxed{p(\\boldf(\\boldy)|\\boldt(\\boldX)) = \\Normal(f(\\boldy)|\\boldmu, \\sigma^2(\\boldy))},\\\\\n",
    "&\\boxed{\\boldmu_{\\boldy|\\boldX} = \\boldmu_{\\boldy} + \\hat{\\boldk}^T(\\boldy) \\hatSigma_{\\boldX}^{-1}(\\boldf(\\boldX) - \\boldmu_{\\boldX})},\\\\\n",
    "&\\boxed{\\sigma_{\\boldy|\\boldX}^2 = \\hat{\\boldK}(0) - \\hat{\\boldk}^T(\\boldy)\\hatSigma_{\\boldX}^{-1}\\hat{\\boldk}(\\boldy)}.\n",
    "\\end{align*}\n",
    "\n",
    "Занимательный факт состоит в том, что при попытке сделать прогноз в точке $\\boldy = \\boldx_i$, т.е. в точке, которая уже есть в выборке $\\boldX$, получим\n",
    "\\begin{align*}\n",
    "&\\boxed{\\boldmu(\\boldx_i) = f(\\boldx_i),\\qquad \\sigma^2(\\boldx_i) = 0}.\n",
    "\\end{align*}\n",
    "Этот результат верен только в том случае, если в выборке $\\boldX$ точка $\\boldx_i$ встречается ровно один раз. Иначе конечный результат будет является определенного рода усреднением всех достпных значений $f(\\boldx)$ в этой точке.\n",
    "\n",
    "Однако если точка $\\boldy$ не совпадает ни с одной из точек $\\boldX$, то\n",
    "\\begin{align*}\n",
    "&\\boxed{\\boldmu_{\\boldy|\\boldX} = \\boldmu_{\\boldy} + \\boldk^T(\\boldy) (\\Sigma_{\\boldX} + \\sigma^2\\boldI_n)^{-1}(\\boldf(\\boldX) - \\boldmu_{\\boldX})},\\\\\n",
    "&\\boxed{\\sigma_{\\boldy|\\boldX} = K(0) + \\sigma^2 - \\boldk^T(\\boldy)(\\Sigma_{\\boldX} + \\sigma^2 \\boldI_n)^{-1}\\boldk(\\boldy)}.\n",
    "\\end{align*}\n",
    "\n",
    "Таким образом, апостериорный прогноз на $f(\\boldy)$ оказывается разрывным в точках выборки $\\boldX$. Чтобы устранить этот разрыв, будем полагать, что \n",
    "\\begin{align*}\n",
    "&\\mu_{\\boldx_i|\\boldX} = \\lim_{\\boldy \\to \\boldx_i} \\mu_{\\boldy|\\boldX},\n",
    "&\\sigma_{\\boldx_i|\\boldX} = \\lim_{\\boldy \\to \\boldx_i} \\sigma_{\\boldy|\\boldX},\n",
    "\\end{align*}\n",
    "т.е. полагать, что последние две формулы применимы в любой точке $\\boldy \\in \\RR^d$. Для случае нескольких точек $\\boldY$ по аналогии получим\n",
    "\\begin{align*}\n",
    "&\\boxed{\\boldmu_{\\boldY|\\boldX} = \\boldmu_{\\boldY} + \\boldK^T (\\Sigma_{\\boldX} + \\sigma^2 I)^{-1}(\\boldf(\\boldX) - \\boldmu_{\\boldX})},\\\\\n",
    "&\\boxed{\\Sigma_{\\boldY|\\boldX} = \\Sigma_{\\boldY} + \\sigma^2 I - \\boldK^T(\\Sigma_{\\boldX} + \\sigma^2 I)^{-1}\\boldK}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что значения для $\\boldmu_{\\boldy|\\boldX}$ и $\\sigma_{\\boldy|\\boldX}^2$ полученны именно для зашумленного процесса $t(\\boldx)$, но нас интересует прогноз на значение $f(\\boldy)$ незашумленного процесса. Несложно заметить, что шум не влияет на мат. ожидание процесса, однако увеличиывает на $\\sigma^2$ дисперсию прогноза. Поэтому для среднего и дисперсии для $f(\\boldY)$ получаем (с уже устраненными разрывами в точках выборки $\\boldX$):\n",
    "\\begin{align*}\n",
    "&\\boxed{p(\\boldf(\\boldY)|\\boldt(\\boldX)) = \\Normal(\\boldf(\\boldY)|\\boldmu_{\\boldY|\\boldX}, \\Sigma_{\\boldY|\\boldX})},\\\\\n",
    "&\\boxed{\\boldmu_{\\boldY|\\boldX} = \\boldmu_{\\boldY} + \\boldK^T (\\Sigma_{\\boldX} + \\sigma^2 I)^{-1}(\\boldf(\\boldX) - \\boldmu_{\\boldX})},\\\\\n",
    "&\\boxed{\\Sigma_{\\boldY|\\boldX} = \\Sigma_{\\boldY} - \\boldK^T(\\Sigma_{\\boldX} + \\sigma^2 I)^{-1}\\boldK}.\n",
    "\\end{align*}\n",
    "Или для одной точки $\\boldy$:\n",
    "\\begin{align*}\n",
    "&\\boxed{p(\\boldf(\\boldy)|\\boldt(\\boldX)) = \\Normal(\\boldf(\\boldy)|\\boldmu_{\\boldy|\\boldX}, \\Sigma_{\\boldy|\\boldX})},\\\\\n",
    "&\\boxed{\\boldmu_{\\boldy|\\boldX} = \\boldmu_{\\boldY} + \\boldk^T (\\Sigma_{\\boldX} + \\sigma^2 I)^{-1}(\\boldf(\\boldX) - \\boldmu_{\\boldX})},\\\\\n",
    "&\\boxed{\\Sigma_{\\boldy|\\boldX} = K(0) - \\boldk^T(\\Sigma_{\\boldX} + \\sigma^2 I)^{-1}\\boldk}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpr_theory_mle'></a>\n",
    "### Выбов параметров ковариационной функции (ядра)<sup>[toc](#toc)</sup>\n",
    "Зачастую ковариационная функция парамеризована некоторым набором параметров. Например, RBF-ядро обладает параметром $\\gamma$ (зачастую именуемым `scale`):\n",
    "$$\n",
    "K(\\boldx', \\boldx'') = \\exp\\lp-\\frac{\\|\\boldx' - \\boldx''\\|_2^2}{2\\gamma}\\rp.\n",
    "$$\n",
    "Соответственно возникает вопрос о том, как выбирать эти параметры. Обычно выбирают параметры, максимизирующие правдоподобие наблюдаемых данных:\n",
    "$$\n",
    "\\boldtheta^* = \\argmax_{\\boldtheta} p(f(\\boldX);\\boldtheta).\n",
    "$$\n",
    "Заметим, что \n",
    "\n",
    "$$\n",
    "K(\\boldx', \\boldx'') = \\exp\\lp-\\frac{\\|\\boldx' - \\boldx''\\|_2^2}{2\\gamma}\\rp.\n",
    "$$\n",
    "\n",
    "Здесь присутствую сразу три параметра: \n",
    "* Коэффициент $C$ перед RBF-ядром\n",
    "* Коэффициент масштабирования $\\gamma$\n",
    "* Дисперсия шума $\\sigma^2$\n",
    "Сложность нахождения этих параметров состоит в том, что логарифм правдоподобия\n",
    "$$\n",
    "\\mathcal{L}(C,\\gamma,\\sigma^2;f(\\boldX))\n",
    "$$\n",
    "обладает множеством локальных максимумов (ниже это будет продемонстриовано).\n",
    "Поэтому зачастую приходится запускать численную процедуру максимизации правдободобия из многих\n",
    "точек в множестве допустимых значений параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpr_practice'></a>\n",
    "# Регрессия на основе гауссовских процессов: практика<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpr_sklearn'></a>\n",
    "## Гауссовские процессы в библиотеке sklearn<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='doc_gpr'></a>\n",
    "### [GuassianProcessRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html)<sup>[toc](#toc)</sup>\n",
    "\n",
    "`GaussianProcessRegressor(kernel=None, alpha=1e-10, optimizer=’fmin_l_bfgs_b’, n_restarts_optimizer=0, normalize_y=False, copy_X_train=True, random_state=None)`\n",
    "\n",
    "Основные параметры GuassianProcessRegrossor, представляющие интерес:\n",
    "* `kernel`:  **kernel object.** The kernel specifying the covariance function of the GP. If None is passed, the kernel “1.0 * RBF(1.0)” is used as default. Note that the kernel’s hyperparameters are optimized during fitting.\n",
    "* `alpha`: **float or array-like, optional (default: 1e-10).** Value added to the diagonal of the kernel matrix during fitting. Larger values correspond to increased noise level in the observations. This can also prevent a potential numerical issue during fitting, by ensuring that the calculated values form a positive definite matrix. If an array is passed, it must have the same number of entries as the data used for fitting and is used as datapoint-dependent noise level. Note that this is equivalent to adding a WhiteKernel with c=alpha. Allowing to specify the noise level directly as a parameter is mainly for convenience and for consistency with Ridge.\n",
    "* `n_restarts_optimizer`: **int, optional (default: 0).** The number of restarts of the optimizer for finding the kernel’s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel’s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer == 0 implies that one run is performed.\n",
    "* `normalize_y`: **boolean, optional (default: False).** Whether the target values y are normalized, i.e., the mean of the observed target values become zero. This parameter should be set to True if the target values’ mean is expected to differ considerable from zero. When enabled, the normalization effectively modifies the GP’s prior based on the data, which contradicts the likelihood principle; normalization is thus disabled per default.\n",
    "* `copy_X_train`: **bool, optional (default: True).** If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally.\n",
    "* `random_state`: **int, RandomState instance or None, optional (default: None).** The generator used to initialize the centers. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpr_sklearn_kernels'></a>\n",
    "### Доступные ядра в библиотеке sklearn<sup>[toc](#toc)</sup>\n",
    "Посмотрим на сечения гауссовских процессов с различными ядрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import (\n",
    "    Kernel,\n",
    "    ConstantKernel,\n",
    "    WhiteKernel,\n",
    "    RBF\n",
    ")\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = [0, 10]\n",
    "n_points = 501\n",
    "x_values = np.linspace(x_range[0], x_range[1], n_points)[:, None]\n",
    "\n",
    "n_plots = 3\n",
    "colors = ['k', 'gray', 'k']\n",
    "linestyles = ['-', '-', '--']\n",
    "plot_params = [{'color': colors[n_plot], 'linestyle': linestyles[n_plot]} \n",
    "               for n_plot in range(n_plots)]\n",
    "\n",
    "kernels = OrderedDict()\n",
    "kernels[0] = RBF(length_scale=1.0)\n",
    "kernels[1] = RBF(length_scale=0.2)\n",
    "kernels[2] = RBF(length_scale=1.0) + WhiteKernel(0.05)\n",
    "kernels[3] = WhiteKernel(1.0)\n",
    "\n",
    "kernel_labels = {0: 'RBF ($\\gamma = 1$)',\n",
    "                 1: 'RBF ($\\gamma = 0.2$)',\n",
    "                 2: 'RBF ($\\gamma=1$) + white noise ($\\sigma^2 = 0.05$)',\n",
    "                 3: 'White noise ($\\sigma^2=1$)'}\n",
    "\n",
    "specific_plot_params = {0: {'linewidth': 1.5}, \n",
    "                        1: {'linewidth': 1.5},\n",
    "                        2: {'linewidth': 0.7},\n",
    "                        3: {'linewidth': 0.7}}\n",
    "                    \n",
    "n_kernels = len(kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_size = 8\n",
    "y_size = 5\n",
    "n_cols = 2\n",
    "n_rows = int(np.ceil(n_kernels / n_cols))\n",
    "fig, axarr = plt.subplots(n_rows, n_cols, figsize=(x_size * n_cols, y_size * n_rows), sharex=True, sharey=True)\n",
    "\n",
    "for n_kernel in range(n_kernels):\n",
    "    ax = axarr[n_kernel // n_cols, n_kernel % n_cols]\n",
    "    kernel = kernels[n_kernel]\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel)\n",
    "    for n_plot in range(n_plots):\n",
    "        y_values = gpr.sample_y(x_values, random_state=n_kernel + n_plot)\n",
    "        _plot_params = copy.deepcopy(plot_params[n_plot])\n",
    "        for key, value in specific_plot_params[n_kernel].items():\n",
    "            _plot_params[key] = value\n",
    "        ax.plot(x_values.ravel(), y_values.ravel(), **_plot_params)\n",
    "    ax.set_title(kernel_labels[n_kernel]);\n",
    "plt.tight_layout()\n",
    "if not DO_NOT_SAVE_IMAGES:\n",
    "    plt.savefig(PICS_DIR + '/gpr_realizations.pdf', dpi=600, format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание обучающей выборки\n",
    "n_train_points = 8                 # Количество точек в обучающей выборке\n",
    "x_train_points_range = [1.5, 8.5]  # Диапазон значений индексирующих параметров в обучающей выборке\n",
    "\n",
    "kernel = RBF(0.5)\n",
    "gpr = GaussianProcessRegressor(kernel)\n",
    "np.random.seed(3)\n",
    "X_train = np.random.uniform(x_train_points_range[0], x_train_points_range[1], size=(n_train_points, 1))\n",
    "y_train = gpr.sample_y(X_train, random_state=4)\n",
    "\n",
    "x_range = [0, 10]       # Диапазон значений для прогноза\n",
    "n_pred_points = 501     # Количество точек, в которых будем делать прогноз\n",
    "X_pred = np.linspace(x_range[0], x_range[1], n_points)[:, None]\n",
    "\n",
    "# Обучение и предсказание\n",
    "gpr.fit(X_train, y_train)\n",
    "y_pred, y_pred_std = gpr.predict(X_pred, return_std=True)\n",
    "\n",
    "# Отрисовка\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(X_train.ravel(), y_train.ravel(), linestyle='none', color='r', marker='o', zorder=2,\n",
    "        label=r'$\\boldsymbol{f}(\\boldsymbol{x})$')\n",
    "plt.plot(X_pred.ravel(), y_pred.ravel(), linestyle='--', color='k', zorder=2,\n",
    "         label=r'$\\mu_{\\boldsymbol{y}|\\boldsymbol{X}}$')\n",
    "plt.fill_between(X_pred.ravel(), y_pred.ravel() + y_pred_std, y_pred.ravel() - y_pred_std,\n",
    "                 color='b', alpha=0.5, zorder=2,\n",
    "                 label=r'$\\mu_{\\boldsymbol{y}|\\boldsymbol{X}} \\pm \\sigma_{\\boldsymbol{y}|\\boldsymbol{X}}^2$')\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5)\n",
    "plt.xlim(x_range); plt.ylim([-2, 2])\n",
    "plt.xlabel(r'$x$'); plt.ylabel(r'$f(x)$')\n",
    "plt.title(r'Noiseless gaussian random process ($\\sigma^2 = 0$)')\n",
    "plt.legend()\n",
    "if not DO_NOT_SAVE_IMAGES:\n",
    "    plt.savefig(PICS_DIR +  '/gpr_noise_free_pred.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание обучающей выборки\n",
    "n_train_points = 8                 # Количество точек в обучающей выборке\n",
    "x_train_points_range = [1.5, 8.5]  # Диапазон значений индексирующих параметров в обучающей выборке\n",
    "\n",
    "kernel = RBF(0.5)\n",
    "gpr = GaussianProcessRegressor(kernel, alpha=0.2)\n",
    "np.random.seed(3)\n",
    "X_train = np.random.uniform(x_train_points_range[0], x_train_points_range[1], size=(n_train_points, 1))\n",
    "y_train = gpr.sample_y(X_train, random_state=4)\n",
    "\n",
    "x_range = [0, 10]       # Диапазон значений для прогноза\n",
    "n_pred_points = 501     # Количество точек, в которых будем делать прогноз\n",
    "X_pred = np.linspace(x_range[0], x_range[1], n_points)[:, None]\n",
    "\n",
    "# Обучение и предсказание\n",
    "gpr.fit(X_train, y_train)\n",
    "y_pred, y_pred_std = gpr.predict(X_pred, return_std=True)\n",
    "\n",
    "# Отрисовка\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(X_train.ravel(), y_train.ravel(), linestyle='none', color='r', marker='o', zorder=2,\n",
    "        label=r'$\\boldsymbol{f}(\\boldsymbol{x})$')\n",
    "plt.plot(X_pred.ravel(), y_pred.ravel(), linestyle='--', color='k', zorder=2,\n",
    "         label=r'$\\mu_{\\boldsymbol{y}|\\boldsymbol{X}}$')\n",
    "plt.fill_between(X_pred.ravel(), y_pred.ravel() + y_pred_std, y_pred.ravel() - y_pred_std,\n",
    "                 color='b', alpha=0.5, zorder=2,\n",
    "                 label=r'$\\mu_{\\boldsymbol{y}|\\boldsymbol{X}} \\pm \\sigma_{\\boldsymbol{y}|\\boldsymbol{X}}^2$')\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5)\n",
    "plt.xlim(x_range); plt.ylim([-2, 2])\n",
    "plt.xlabel(r'$x$'); plt.ylabel(r'$f(x)$')\n",
    "plt.legend();\n",
    "plt.title(r'Gaussian random process with normal noise ($\\sigma^2 = 0.2$)')\n",
    "if not DO_NOT_SAVE_IMAGES:\n",
    "    plt.savefig(PICS_DIR + '/gpr_noisy_pred.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что в точках обучающей выборки $\\boldX$ прогноз строится как будто-то бы \"с нуля\", в противном случае в точках выборке апостериорное среднее $\\mu_{\\boldy|\\boldX}$ было бы разрывно. Проблема устранения разрыва ранее была рассмотрена теоретически."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpr_sustainability'></a>\n",
    "## Устойчивость модели гауссовской регрессии<sup>[toc](#toc)</sup>\n",
    "Гауссовская регрессия без шума является крайне нестабильным алгоритмом для предсказания значений. Разберемся, почему.\n",
    "\n",
    "Пусть в обучающей выборке $\\boldX$ присутствуют два близкие точки $\\boldx_i$ и $\\boldx_j$, но их значения $f(\\boldx_i)$ и $f(\\boldx_j)$ существенно отличаются, т.е. реализация процесса претерпевает скачок в окрестности данных точек. В таком случае при максимизаци правдоподобия модель вынуждена подстроиться, чтобы объяснить этот резкий перепад значений. Резкий перепад в отсутствие шума может объяснить лишь быстро убывающая функция ковариации, но такие функцие ковариации практически бесполезны, так как они позволяют предсказать значения лишь в точках $\\boldY$, близких к точкам обучающей выборки $\\boldX$. Ниже наглядно изображено, что может произойти при наличии в данных хотя бы одного выброса. Выброс здесь генерируем сами. \n",
    "\n",
    "Разумеется, если данные действительно соответствует модели гауссовой регрессии без шума, то такого резкого перепада в них почти никогда не будет. Но на практике крайне редко можно наблюдать данные, о которых можно сказать, что в них гарантированно нет шума."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании библиотеки `sklearn` мы работаем с классом `GaussianProcessRegressor`. Шум в данной модели можно задать двумя способами: как параметр ядра или как параметров `alpha` в конструкторе объекта.\n",
    "\n",
    "Рассмотрим процесс подбора параметров в случае RBF-ядра:\n",
    "$$\n",
    "K(\\boldx', \\boldx'') = \\exp \\lp -\\frac{\\|\\boldx' - \\boldx''\\|_2^2}{2\\gamma} \\rp.\n",
    "$$\n",
    "Чем \"резче\" упомянутый выше скачок между двумя соседними точками, тем меньшее значение параметра `lower_scale` ($\\gamma$) подберет процедура максимизации правдоподобия. В таком случае будет получена быстро убывающая функция ковариации, практически не объясняющая зависимость между данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание обучающей выборки с двуями близкими индексирующими параметрами\n",
    "n_train_points = 8                 # Количество точек в обучающей выборке\n",
    "x_train_points_range = [1.5, 8.5]  # Диапазон значений индексирующих параметров в обучающей выборке\n",
    "y_range = [-2, 2.5]\n",
    "\n",
    "real_scale = 1  # Истинный параметр ковариационной функции, которая порождает обучающую выборку\n",
    "real_alpha = 0.00001\n",
    "\n",
    "fig, axarr = plt.subplots(2, 1, figsize=(20, 16))\n",
    "x_new_value = None\n",
    "\n",
    "for n_plot, add_noisy_point in enumerate([True, False]):\n",
    "    # Диапазон значений scale-а: (1e-3, 1). Изначально генерируем данные для scale=1\n",
    "    kernel = RBF(real_scale, length_scale_bounds=(1e-3, 5))\n",
    "    gpr = GaussianProcessRegressor(kernel, alpha=real_alpha, n_restarts_optimizer=10)\n",
    "    np.random.seed(3)\n",
    "    X_train = np.random.uniform(x_train_points_range[0], x_train_points_range[1], size=(n_train_points, 1))\n",
    "    y_train = gpr.sample_y(X_train, random_state=4)\n",
    "\n",
    "    if add_noisy_point:\n",
    "        x_new_value = X_train[0, 0] + 0.01\n",
    "        y_new_value = y_train[0, 0] - 0.1\n",
    "\n",
    "        X_train = np.vstack([X_train, np.array([[x_new_value]])])\n",
    "        y_train = np.vstack([y_train, np.array([[y_new_value]])])\n",
    "\n",
    "    x_range = [0, 10]       # Диапазон значений для прогноза\n",
    "    n_pred_points = 501     # Количество точек, в которых будем делать прогноз\n",
    "    X_pred = np.linspace(x_range[0], x_range[1], n_points)[:, None]\n",
    "    gpr.fit(X_train, y_train)\n",
    "    print('Fitted kernel:', gpr.kernel_)\n",
    "    y_pred, y_pred_std = gpr.predict(X_pred, return_std=True)\n",
    "\n",
    "    # Отрисовка\n",
    "    axarr[n_plot].plot(X_train.ravel(), y_train.ravel(), linestyle='none', color='r', marker='o', zorder=2,\n",
    "             label=r'$\\boldsymbol{f}(\\boldsymbol{x})$')\n",
    "    axarr[n_plot].plot(X_pred.ravel(), y_pred.ravel(), linestyle='--', color='k', zorder=2,\n",
    "                       label=r'$\\mu_{\\boldsymbol{y}|\\boldsymbol{X}}$')\n",
    "    axarr[n_plot].fill_between(X_pred.ravel(), y_pred.ravel() + y_pred_std, y_pred.ravel() - y_pred_std,\n",
    "                     color='b', alpha=0.5, zorder=2,\n",
    "                               label=r'$\\mu_{\\boldsymbol{y}|\\boldsymbol{X}} \\pm \\sigma_{\\boldsymbol{y}|\\boldsymbol{X}}^2$')\n",
    "    axarr[n_plot].axvline(x_new_value, y_range[0], y_range[1], color='b', linestyle='--')\n",
    "    axarr[n_plot].grid(which='both', linestyle='--', alpha=0.5)\n",
    "    axarr[n_plot].set_xlim(x_range)\n",
    "    axarr[n_plot].set_ylim(y_range)\n",
    "    axarr[n_plot].set_xlabel(r'$x$')\n",
    "    axarr[n_plot].set_ylabel(r'$f(x)$')\n",
    "    axarr[n_plot].set_title('$\\gamma_{{real}} = {},\\quad \\gamma_{{MLE}} = {:.3f},\\quad \\sigma^2 = {:.2f}$'\n",
    "                            .format(real_scale, gpr.kernel_.length_scale, real_alpha))\n",
    "    axarr[n_plot].legend()\n",
    "\n",
    "if not DO_NOT_SAVE_IMAGES:\n",
    "    if alpha <= 1e-10:\n",
    "        plt.savefig(PICS_DIR + '/gpr_singularity.pdf')\n",
    "    else:\n",
    "        plt.savefig(PICS_DIR + '/gpr_singularity_solved.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, даже одна зашумленная точка может сломать гауссовскую регрессию. В то же время достаточно предположить наличие шума, как ситуация выправляется: поставьте выше `real_alpha = 0.05` и убедитесь в этом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpr_example1'></a>\n",
    "## Восстановление зависимостей с помощью гауссовской регрессии<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы рассмотрим гауссовскую регрессию для аппроксимации следующих функций:\n",
    "$$\n",
    "f_1(x) = \\cos(x),\\\\\n",
    "f_2(x) = x \\sin(x), \\\\\n",
    "f_3(x) = \n",
    "\\begin{cases} 5 - x^2, x < 2 \\\\ 13x - 1, x \\ge 2 \n",
    "\\end{cases}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "def f2(x):\n",
    "    return x * np.sin(x)\n",
    "\n",
    "def f3(x):\n",
    "    return np.array([5 - x[i] ** 2 if x[i] < 2 else 13 * x[i] - 1 for i in range(len(x))])\n",
    "\n",
    "FUNCTIONS_SET = {'f1': f1, \n",
    "                 'f2': f2,\n",
    "                 'f3': f3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_it(x, function, X, y, y_pred, dy=None, sigma=1.0, alpha=0.05, ax=None, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    -x: array-like\n",
    "        Points at which we compare our predictions (y_pred) with real values of the estimated function.\n",
    "    -function: callable\n",
    "        Real function which are being estimated. \n",
    "    -X: array-like\n",
    "        These are x points of the training set we have.\n",
    "    -y: array-like\n",
    "        These are y values of the training set we have.\n",
    "    -y_pred: These are predictions we have made based on GPR.\n",
    "    -dy:      Real variance of the noise at points X\n",
    "    -sigma:   Noise variance.\n",
    "    -alpha:   It is the parameter of the confidence interval.\n",
    "    -ax:      Axis to plot the data on.\n",
    "    -figsize: \n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    # Plotting true function at requested points\n",
    "    ax.plot(x, function(x), 'r:', label=u'true function', zorder=2) \n",
    "    ax.plot(X, y, 'r.', markersize=6, label=u'Observations/Constraints', zorder=2)\n",
    "    # Plotting predicted samples at requested points\n",
    "    ax.plot(x, y_pred, 'b-', label=u'Prediction', zorder=2) \n",
    "    if dy is not None:\n",
    "        # Plotting real noise variance at training points x\n",
    "        plt.errorbar(X.ravel(), y, dy, fmt='r.', markersize=6, label=u'Real noise variance $\\\\sigma^2(x)$')\n",
    "        \n",
    "    plt.fill(np.concatenate([x, x[::-1]]),\n",
    "             np.concatenate([y_pred + scipy.stats.norm.ppf(alpha / 2) * sigma,\n",
    "                            (y_pred + scipy.stats.norm.ppf(1 - alpha / 2) * sigma)[::-1]]),\n",
    "             alpha=.5, fc='b', ec='None', label=r'95\\% confidence interval')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$f(x)$')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(which='both', linestyle='--', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример без случайного шума (Noise-free case)<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FUNCTIONS_SET['f1']\n",
    "X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T # add point 12\n",
    "y = np.array(f(X)).ravel()\n",
    "\n",
    "x = np.atleast_2d(np.linspace(-8, 18, 1000)).T # increase the range\n",
    "\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=1)\n",
    "gpr.fit(X, y)\n",
    "y_pred, sigma = gpr.predict(x, return_std=True)\n",
    "plot_it(x=x, function=f, X=X, y=y, y_pred=y_pred, sigma=sigma, figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FUNCTIONS_SET['f2']\n",
    "X = np.atleast_2d([-7, -3, -1, 0, 2, 4, 6]).T\n",
    "y = np.array(f(X)).ravel()\n",
    "\n",
    "x = np.atleast_2d(np.arange(-12, 12, 0.1)).T\n",
    "\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 5e1))\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=5)\n",
    "gpr.fit(X, y)\n",
    "y_pred, sigma = gpr.predict(x, return_std=True)\n",
    "\n",
    "plot_it(x=x, function=f, X=X, y=y, y_pred=y_pred, sigma=sigma, figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FUNCTIONS_SET['f3']\n",
    "X = np.atleast_2d([-7, -3, -1, 0, 2, 4, 6]).T\n",
    "y = np.array(f(X)).ravel()\n",
    "\n",
    "x = np.atleast_2d(np.arange(-15, 15, 0.1)).T\n",
    "\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 5e1))\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "gpr.fit(X, y)\n",
    "y_pred, sigma = gpr.predict(x, return_std=True)\n",
    "\n",
    "plot_it(x=x, function=f, X=X, y=y, y_pred=y_pred, sigma=sigma, figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример со случайным шумом (Noisy case)<sup>[toc](#toc)</sup>\n",
    "Стоит умпомянуть, что параметр `alpha` у `GuassianProcessRegressor` не обязательно должен быть скаляром: он может быть задан вектором, каждая из компонент которого показывает уровень шума в конкретной точке обучающей выборки. И действительно, в выкладках, касающихся гауссовской регрессии с шумом, присутствует матрица\n",
    "$$\n",
    "\\hatSigma_{\\boldX} = \\Sigma_{\\boldX} + \\sigma^2 I.\n",
    "$$\n",
    "В то же время в выкладках ничего не изменится, если заменить фиксированную дисперсию $\\sigma^2$ в каждой точке на зависящую от точки $\\boldx$ величину $\\sigma^2(\\boldx)$:\n",
    "$$\n",
    "\\hatSigma_{\\boldX} = \\Sigma_{\\boldX} + \\diag(\\sigma^2(\\boldx_1), \\dots, \\sigma^2(\\boldx_n)) = \\Sigma_{\\boldX} + \\Lambda.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FUNCTIONS_SET['f2']\n",
    "X = np.linspace(0, 10, 20) # x samples used for training\n",
    "X = np.atleast_2d(X).T\n",
    "y = np.array(f(X)).ravel()\n",
    "\n",
    "# Now we generate the noise\n",
    "np.random.seed(1)\n",
    "dy = 0.5 + .1 * np.random.random(y.shape)\n",
    "noise = np.random.normal(0, dy) # generating noise\n",
    "y = y + noise                   # adding noise to the points\n",
    "\n",
    "x = np.atleast_2d(np.linspace(-2, 12, 1000)).T # increase the range\n",
    "\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=dy**2, n_restarts_optimizer=10, random_state=1)\n",
    "gp.fit(X, y)\n",
    "y_pred_noise, sigma_noise = gp.predict(x, return_std=True)\n",
    "\n",
    "plot_it(x=x, function=f, X=X, y=y, y_pred=y_pred_noise, sigma=sigma_noise, dy=dy, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpr_ard'></a>\n",
    "## Automatic relevance determination (ARD)<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import (\n",
    "    Kernel,\n",
    "    ConstantKernel,\n",
    "    WhiteKernel,\n",
    "    RBF\n",
    ")\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "N = 51\n",
    "noise_std = 0.2\n",
    "\n",
    "x1 = np.linspace(0, 5, N)\n",
    "x2 = np.array(x1) + np.random.normal(size=x1.shape)\n",
    "x3 = np.random.normal(size=N) * np.max(x1)\n",
    "y = np.sin(2 * np.pi * x1) + np.random.normal(scale=noise_std, size=N)\n",
    "X = np.vstack([x1, x2, x3]).T\n",
    "assert X.shape == (N, 3)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x1, np.sin(2 * np.pi * x1), color='r', label='gt');\n",
    "plt.plot(x1, y, color='b', label='noisy');\n",
    "plt.xlabel('$x_1$'); plt.ylabel('$y$');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ConstantKernel() * RBF(length_scale=(1., 1., 1.)) + WhiteKernel(1.)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel)\n",
    "gpr.fit(X, y)\n",
    "gpr.kernel_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpr_mle_visualization'></a>\n",
    "## Визуализация правдоподобия<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание обучающей выборки\n",
    "n_train_points = 20                # Количество точек в обучающей выборке\n",
    "x_train_points_range = [1.5, 8.5]  # Диапазон значений индексирующих параметров в обучающей выборке\n",
    "\n",
    "real_noise_level = 0.25\n",
    "real_scale = 0.25\n",
    "kernel = RBF(real_scale, (1e-3, 1e4)) +\\\n",
    "         WhiteKernel(noise_level=real_noise_level, noise_level_bounds=(1e-10, 1e4))\n",
    "\n",
    "gpr = GaussianProcessRegressor(kernel, n_restarts_optimizer=1)\n",
    "np.random.seed(3)\n",
    "X_train = np.random.uniform(x_train_points_range[0], x_train_points_range[1], size=(n_train_points, 1))\n",
    "y_train = gpr.sample_y(X_train, random_state=4)\n",
    "\n",
    "x_range = [0, 10]       # Диапазон значений для прогноза\n",
    "n_pred_points = 501     # Количество точек, в которых будем делать прогноз\n",
    "X_pred = np.linspace(x_range[0], x_range[1], n_points)[:, None]\n",
    "\n",
    "# Обучение и предсказание\n",
    "gpr.fit(X_train, y_train)\n",
    "print('Fitted kernel:', gpr.kernel_)\n",
    "print('Fitted loglikelihood:', gpr.log_marginal_likelihood_value_)\n",
    "y_pred, y_pred_std = gpr.predict(X_pred, return_std=True)\n",
    "\n",
    "# Отрисовка аппроксимации\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(X_train.ravel(), y_train.ravel(), linestyle='none', color='r', marker='o', zorder=2,\n",
    "        label=r'$\\boldsymbol{f}(\\boldsymbol{x})$')\n",
    "plt.plot(X_pred.ravel(), y_pred.ravel(), linestyle='--', color='k', zorder=2,\n",
    "         label=r'$\\mu_{\\boldsymbol{y}|\\boldsymbol{X}}$')\n",
    "plt.fill_between(X_pred.ravel(), y_pred.ravel() + y_pred_std, y_pred.ravel() - y_pred_std,\n",
    "                 color='b', alpha=0.5, zorder=2,\n",
    "                 label=r'$\\mu_{\\boldsymbol{y}|\\boldsymbol{X}} \\pm \\sigma_{\\boldsymbol{y}|\\boldsymbol{X}}^2$')\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5)\n",
    "plt.xlim(x_range)\n",
    "plt.ylim([-2, 2])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$f(x)$')\n",
    "plt.title(r'Gaussian random process ($\\sigma^2 = {}$)'.format(real_noise_level))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем значения возможных правдоподобий при различных параметрах ядра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "# Отрисовка логарифма правдоподобия\n",
    "scale_range = np.logspace(-5, 5, 150)  # Диапазон значений scale-а\n",
    "sigma2_range = np.logspace(-7, 5, 150) # Диапазон значений sigma^2\n",
    "log_scale_range, log_sigma2_range = np.meshgrid(scale_range, sigma2_range)\n",
    "LML = [[gpr.log_marginal_likelihood(np.log([log_scale_range[i, j], log_sigma2_range[i, j]]))\n",
    "        for i in range(scale_range.shape[0])] for j in range(sigma2_range.shape[0])]\n",
    "LML = np.array(LML).T\n",
    "vmin, vmax = (-LML).min(), (-LML).max()\n",
    "vmax = 50\n",
    "level = np.around(np.logspace(np.log10(vmin), np.log10(vmax), 50), decimals=1)\n",
    "plt.contour(log_scale_range, log_sigma2_range, -LML,\n",
    "            levels=level, norm=LogNorm(vmin=vmin, vmax=vmax))\n",
    "plt.colorbar()\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Length-scale\")\n",
    "plt.ylabel(\"Noise-level\")\n",
    "plt.title(\"Log-marginal-likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь имеется как минимум два локальных минимума, в которые может попасть процесс оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из графика видно, что адаптивный дизайн на порядки превосходит случайный равномерный дизайн. Что же касается OLHS, то данный метод в среднем ведет себя лучше случайного равномерно дизайна, однако его превосходство не столь значительно, как у адаптивного дизайна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='automl'></a>\n",
    "# AutoML<sup><a href='#toc'>toc</a></sup>\n",
    "* [Linear SVM](#automl_linear_svm)\n",
    "* [TASK. Kernel SVM](#automl_kernel_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=20, n_informative=15, n_redundant=5,\n",
    "                           flip_y=0.05, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='automl_linear_svm'></a>\n",
    "## Linear SVM<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_linear_svc(C, X_train, y_train, X_test, y_test, random_state=2, *args, **kwargs):\n",
    "    svc = LinearSVC(C=C, random_state=random_state)\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_test_pred = svc.decision_function(X_test)\n",
    "    return roc_auc_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "score = estimate_linear_svc(1.0, X_train, y_train, X_test, y_test)\n",
    "print('Initial score = {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прямой перебор (grid search)<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "C_range = np.logspace(-4, 1, 201)\n",
    "scores = []\n",
    "for C in C_range:\n",
    "    scores.append(estimate_linear_svc(C, X_train, y_train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best score = {} at C = {}'.format(np.max(scores), C_range[np.argmax(scores)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(C_range, scores, marker='x')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('ROC\\_AUC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Байесовский подход (bayesian optimization)<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = np.logspace(-4, 1, 201)\n",
    "np.random.seed(10)\n",
    "\n",
    "hypoparams = list(np.random.choice(C_range, size=5, replace=False))\n",
    "scores = []\n",
    "for C in hypoparams:\n",
    "    scores.append(estimate_linear_svc(C, X_train, y_train, X_test, y_test))\n",
    "\n",
    "best_C = C_range[np.argmax(scores)]\n",
    "best_score = np.max(scores)\n",
    "\n",
    "max_iters = 100\n",
    "stop_iters = 20\n",
    "senseless_iters = 0\n",
    "\n",
    "kernel = WhiteKernel() + RBF()\n",
    "\n",
    "for n_iter in range(max_iters):\n",
    "    gpr = GaussianProcessRegressor(kernel, random_state=5, n_restarts_optimizer=10)\n",
    "    gpr.fit(np.log(np.array(hypoparams)[:, None]), scores)\n",
    "    pred, stds = gpr.predict(np.log(C_range[:, None]), return_std=True)\n",
    "    C = C_range[np.argmax(stds)]\n",
    "    print('{}: variance for C={} is {}'.format(n_iter+1, C, np.max(stds)))\n",
    "\n",
    "    score = estimate_linear_svc(C, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    scores.append(score)\n",
    "    hypoparams.append(C)\n",
    "\n",
    "    if score > best_score:\n",
    "        best_C = C\n",
    "        best_score = score\n",
    "        print(best_C, best_score)\n",
    "        senseless_iters = 0\n",
    "    else:\n",
    "        senseless_iters += 1\n",
    "        if senseless_iters == stop_iters:\n",
    "            print('Stop since there are not imporevement for {}'.format(senseless_iters))\n",
    "            break\n",
    "    \n",
    "print(best_C, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='automl_kernel_svm'></a>\n",
    "## TASK. Kernel SVM<sup>[toc](#toc)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=500, n_features=20, n_informative=15, n_redundant=5,\n",
    "                           flip_y=0.05, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=125, random_state=1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_kernel_svm(C, gamma, X_train, y_train, X_test, y_test, random_state=2, *args, **kwargs):\n",
    "    svc = SVC(C=C, gamma=gamma, random_state=random_state)\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_test_pred = svc.decision_function(X_test)\n",
    "    return roc_auc_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values_number = 71\n",
    "gamma_values_number = 71\n",
    "\n",
    "C_range = np.logspace(-4, 3, C_values_number)\n",
    "gamma_range = np.logspace(-4, 3, gamma_values_number)\n",
    "scores = np.zeros((C_values_number, gamma_values_number))\n",
    "\n",
    "best_C = -1\n",
    "best_gamma = -1\n",
    "best_score = -1\n",
    "\n",
    "for C_index, gamma_index in product(range(C_values_number), range(gamma_values_number)):\n",
    "    C = C_range[C_index]\n",
    "    gamma = gamma_range[gamma_index]\n",
    "    score = estimate_kernel_svm(C, gamma, X_train, y_train, X_test, y_test)\n",
    "    scores[gamma_index, C_index] = score\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_C = C\n",
    "        best_gamma = gamma\n",
    "print('C={}, gamma={} -> {}'.format(best_C, best_gamma, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XX, YY = np.meshgrid(C_range, gamma_range)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.pcolormesh(XX, YY, scores, cmap='afmhot')\n",
    "plt.colorbar()\n",
    "plt.scatter([best_C], [best_gamma], marker='*', color='r', s=100)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('$C$')\n",
    "plt.ylabel('$\\gamma$');"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
